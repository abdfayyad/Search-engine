{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "import pickle\n",
    "from typing import List\n",
    "import numpy as np\n",
    "from nltk import tokenize, pos_tag, download\n",
    "from nltk.corpus import stopwords, wordnet\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import ir_datasets\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import pandas as pd\n",
    "from typing import Callable\n",
    "import gensim\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.models.callbacks import CallbackAny2Vec\n",
    "import logging\n",
    "\n",
    "\n",
    "class LemmatizerWithPOSTagger(WordNetLemmatizer):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def _get_wordnet_pos(self, tag: str) -> str:\n",
    "        if tag.startswith('J'):\n",
    "            return wordnet.ADJ\n",
    "        elif tag.startswith('V'):\n",
    "            return wordnet.VERB\n",
    "        elif tag.startswith('N'):\n",
    "            return wordnet.NOUN\n",
    "        elif tag.startswith('R'):\n",
    "            return wordnet.ADV\n",
    "        else:\n",
    "            return wordnet.NOUN\n",
    "\n",
    "    def lemmatize(self, word: str, pos: str = \"n\") -> str:\n",
    "        return super().lemmatize(word, self._get_wordnet_pos(pos))\n",
    "\n",
    "class TextPreprocessor():\n",
    "\n",
    "    def __init__(self, tokenizer: Callable = None) -> None:\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "        if self.tokenizer is None:\n",
    "            self.tokenizer = tokenize.word_tokenize\n",
    "\n",
    "        self.stopwords_tokens = stopwords.words('english')\n",
    "        self.stemmer = PorterStemmer()\n",
    "        self.lemmatizer = LemmatizerWithPOSTagger()\n",
    "\n",
    "    def tokenize(self, text: str)-> List[str]:\n",
    "        tokens =self.tokenizer(text)\n",
    "        return tokens\n",
    "    \n",
    "    def to_lower(self, tokens: List[str]) -> List[str]:\n",
    "        lower_tokens = []\n",
    "        for token in tokens:\n",
    "            lower_token = str(np.char.lower(token))\n",
    "            lower_tokens.append(lower_token)\n",
    "        return lower_tokens\n",
    "\n",
    "    \n",
    "    def remove_markers(self, tokens: List[str]) -> List[str]:\n",
    "        new_tokens = []\n",
    "        for token in tokens:\n",
    "            new_tokens.append(re.sub(r'\\u00AE', '', token))\n",
    "        return new_tokens\n",
    "\n",
    "    def remove_punctuation(self, tokens: List[str]) ->  List[str]:\n",
    "        new_tokens = []\n",
    "        for token in tokens:\n",
    "            new_tokens.append(token.translate(str.maketrans('', '', string.punctuation)))\n",
    "        return new_tokens\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def rplace_under_score_with_space(self, tokens: List[str]) -> List[str]:\n",
    "        new_tokens = []\n",
    "        for token in tokens:\n",
    "            new_tokens.append(re.sub(r'_', ' ', token))\n",
    "        return new_tokens\n",
    "\n",
    "    def remove_stop_words(self,tokens: List[str]) -> List[str]:\n",
    "        new_tokens = []\n",
    "        for token in tokens:\n",
    "            if token not in self.stopwords_tokens and len(token) > 1:\n",
    "                new_tokens.append(token)\n",
    "        return new_tokens\n",
    "\n",
    "    def remove_apostrophe(self, tokens: List[str]) -> List[str]:\n",
    "        new_tokens = []\n",
    "        for token in tokens:\n",
    "            new_tokens.append(str(np.char.replace(token, \"'\", \" \")))\n",
    "        return new_tokens\n",
    "\n",
    "    def stemming(self, tokens: List[str]) -> List[str]:\n",
    "        new_tokens = []\n",
    "        for token in tokens:\n",
    "            new_tokens.append(self.stemmer.stem(token))\n",
    "        return new_tokens\n",
    "    \n",
    "    \n",
    "    def normalize_appreviations(self, tokens: List[str]) -> List[str]:\n",
    "        new_tokens = []\n",
    "        resolved_terms = {}\n",
    "        for token in tokens:\n",
    "\n",
    "            if len(token) >= 2:\n",
    "                synsets = wordnet.synsets(token)\n",
    "                if synsets:\n",
    "                    resolved_term = synsets[0].lemmas()[0].name()\n",
    "                    resolved_terms[token] = resolved_term\n",
    "\n",
    "        for abbreviation, resolved_term in resolved_terms.items():\n",
    "            for i in range(len(tokens)):\n",
    "                if tokens[i] == abbreviation:\n",
    "                    tokens[i] = resolved_term\n",
    "                    break\n",
    "\n",
    "        return tokens\n",
    "    \n",
    "    def lemmatizing(self, tokens: List[str]) -> List[str]:\n",
    "        tagged_tokens = pos_tag(tokens)\n",
    "        lemmatized_tokens = [self.lemmatizer.lemmatize(token, pos) for token, pos in tagged_tokens]\n",
    "        return lemmatized_tokens\n",
    "    def replace_country_symbols(self, tokens: List[str]) -> List[str]:\n",
    "            country_symbols = {\n",
    "                'US': 'United States', 'UK': 'United Kingdom', 'IN': 'India', 'CA': 'Canada',\n",
    "                'AU': 'Australia', 'DE': 'Germany', 'FR': 'France', 'ES': 'Spain', 'IT': 'Italy',\n",
    "                'JP': 'Japan', 'CN': 'China', 'BR': 'Brazil', 'RU': 'Russia', 'MX': 'Mexico',\n",
    "                'ZA': 'South Africa', 'KR': 'South Korea', 'AR': 'Argentina', 'SA': 'Saudi Arabia',\n",
    "                'EG': 'Egypt', 'NG': 'Nigeria', 'TR': 'Turkey', 'NL': 'Netherlands', 'SE': 'Sweden',\n",
    "                'CH': 'Switzerland', 'BE': 'Belgium', 'AT': 'Austria', 'DK': 'Denmark', 'FI': 'Finland',\n",
    "                'NO': 'Norway', 'PL': 'Poland', 'IE': 'Ireland', 'NZ': 'New Zealand', 'SG': 'Singapore',\n",
    "                'MY': 'Malaysia', 'TH': 'Thailand', 'PH': 'Philippines', 'ID': 'Indonesia', 'VN': 'Vietnam',\n",
    "                'PK': 'Pakistan', 'BD': 'Bangladesh', 'IR': 'Iran', 'IQ': 'Iraq', 'IL': 'Israel', 'GR': 'Greece',\n",
    "                'PT': 'Portugal', 'CZ': 'Czech Republic', 'HU': 'Hungary', 'RO': 'Romania', 'BG': 'Bulgaria',\n",
    "                'HR': 'Croatia', 'SI': 'Slovenia', 'SK': 'Slovakia', 'UA': 'Ukraine', 'BY': 'Belarus', 'LT': 'Lithuania',\n",
    "                'LV': 'Latvia', 'EE': 'Estonia', 'IS': 'Iceland', 'MT': 'Malta', 'CY': 'Cyprus', 'LK': 'Sri Lanka',\n",
    "                'KE': 'Kenya', 'GH': 'Ghana', 'UG': 'Uganda', 'TZ': 'Tanzania', 'SN': 'Senegal', 'DZ': 'Algeria',\n",
    "                'MA': 'Morocco', 'TN': 'Tunisia', 'AE': 'United Arab Emirates', 'QA': 'Qatar', 'KW': 'Kuwait',\n",
    "                'OM': 'Oman', 'BH': 'Bahrain', 'LB': 'Lebanon', 'JO': 'Jordan', 'SY': 'Syria', 'YE': 'Yemen',\n",
    "                'AF': 'Afghanistan', 'UZ': 'Uzbekistan', 'KZ': 'Kazakhstan', 'KG': 'Kyrgyzstan', 'TJ': 'Tajikistan',\n",
    "                'TM': 'Turkmenistan', 'MN': 'Mongolia', 'KH': 'Cambodia', 'LA': 'Laos', 'MM': 'Myanmar', 'NP': 'Nepal',\n",
    "                'BT': 'Bhutan', 'LK': 'Sri Lanka', 'MV': 'Maldives', 'BN': 'Brunei', 'MO': 'Macau', 'HK': 'Hong Kong',\n",
    "                'TW': 'Taiwan', 'AM': 'Armenia', 'GE': 'Georgia', 'AZ': 'Azerbaijan'\n",
    "            }\n",
    "            return [country_symbols.get(token, token) for token in tokens]\n",
    "\n",
    "    def process_hashtags_mentions(self, tokens: List[str]) -> List[str]:\n",
    "            new_tokens = [token for token in tokens if not token.startswith('#') and not token.startswith('@')]\n",
    "            return new_tokens\n",
    "\n",
    "    def normalize_abbreviations(self, tokens: List[str]) -> List[str]:\n",
    "            new_tokens = []\n",
    "            resolved_terms = {}\n",
    "            for token in tokens:\n",
    "                if len(token) >= 2:\n",
    "                    synsets = wordnet.synsets(token)\n",
    "                    if synsets:\n",
    "                        resolved_term = synsets[0].lemmas()[0].name()\n",
    "                        resolved_terms[token] = resolved_term\n",
    "\n",
    "            for abbreviation, resolved_term in resolved_terms.items():\n",
    "                tokens = [resolved_term if token == abbreviation else token for token in tokens]\n",
    "            return tokens\n",
    "    def preprocess(self, text: str) -> str:\n",
    "        operations = [\n",
    "            self.process_hashtags_mentions,       # Step 5: Process hashtags and mentions\n",
    "            self.replace_country_symbols,         # Step 6: Replace country symbols\n",
    "            self.normalize_abbreviations,         # Step 2: Normalize abbreviations\n",
    "            self.remove_markers,                  # Step 9: Remove markers\n",
    "            self.to_lower,                        # Step 1: Convert text to lower case\n",
    "            self.remove_punctuation,              # Step 3: Remove punctuation\n",
    "            self.remove_apostrophe,               # Step 4: Remove apostrophes\n",
    "            self.remove_stop_words,               # Step 8: Remove stop words\n",
    "            self.lemmatizing, \n",
    "        ]\n",
    "        text_tokens=self.tokenize(text)\n",
    "        for op in operations:\n",
    "              text_tokens=op(text_tokens)\n",
    "    \n",
    "        new_text=\"\"\n",
    "        new_text = ' '.join(text_tokens)\n",
    "            \n",
    "        return new_text\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class TfidfEngine:\n",
    "    def __init__(self, text_preprocessor):\n",
    "        self.text_preprocessor = text_preprocessor\n",
    "        self.tfidf_matrix = None\n",
    "        self.tfidf_model = None\n",
    "        self.document_id_mapping = {}\n",
    "\n",
    "    def train_model(self, documents):\n",
    "        document_texts = [doc['text'] for doc in documents]\n",
    "        vectorizer = TfidfVectorizer(preprocessor=self.text_preprocessor.preprocess, tokenizer=self.text_preprocessor.tokenizer)\n",
    "        tfidf_matrix = vectorizer.fit_transform(document_texts)\n",
    "        self.tfidf_matrix = tfidf_matrix\n",
    "        self.tfidf_model = vectorizer\n",
    "        self.save_model(documents)\n",
    "\n",
    "    def save_model(self, documents):\n",
    "        with open('tfidf_model2.pickle', 'wb') as f_model:\n",
    "            pickle.dump(self.tfidf_model, f_model)\n",
    "        with open('tfidf_matrix2.pickle', 'wb') as f_matrix:\n",
    "            pickle.dump(self.tfidf_matrix, f_matrix)\n",
    "        with open('document_id_mapping2.pickle', 'wb') as f_mapping:\n",
    "            pickle.dump({doc['id']: doc['text'] for doc in documents}, f_mapping)\n",
    "\n",
    "    def load_model(self):\n",
    "        with open('tfidf_model2.pickle', 'rb') as f_model:\n",
    "            self.tfidf_model = pickle.load(f_model)\n",
    "        with open('tfidf_matrix2.pickle', 'rb') as f_matrix:\n",
    "            self.tfidf_matrix = pickle.load(f_matrix)\n",
    "        with open('document_id_mapping2.pickle', 'rb') as f_mapping:\n",
    "            self.document_id_mapping = pickle.load(f_mapping)\n",
    "\n",
    "    def query(self, query_text):\n",
    "        preprocessed_query = self.text_preprocessor.preprocess(query_text)\n",
    "        query_vector = self.tfidf_model.transform([preprocessed_query])\n",
    "        return query_vector\n",
    "    \n",
    "    def rank_documents(self, query_vector):\n",
    "        cosine_similarities = cosine_similarity(query_vector, self.tfidf_matrix).flatten()\n",
    "        ranked_indices = np.argsort(-cosine_similarities)\n",
    "        return ranked_indices, cosine_similarities\n",
    "\n",
    "    def get_results(self, query_text):\n",
    "        query_vector = self.query(query_text)\n",
    "        ranked_indices, similarities = self.rank_documents(query_vector)\n",
    "        result_ids = []\n",
    "        for idx in ranked_indices[:10]:  # Top 10 results\n",
    "            if similarities[idx] >= 0.35:\n",
    "                result_ids.append(list(self.document_id_mapping.keys())[idx])\n",
    "        unordered_results = [{'_id': doc_id, 'text': self.document_id_mapping[doc_id]} for doc_id in result_ids]\n",
    "        return unordered_results\n",
    "\n",
    "def calculate_MRR(query_id,tfidf_engine):\n",
    "    relevant_docs = []\n",
    "    for qrel in dataset.qrels_iter():\n",
    "        if qrel[0] == query_id :\n",
    "            relevant_docs.append(qrel[1]) \n",
    "            \n",
    "    ordered_results = []\n",
    "    for query in dataset.queries_iter():\n",
    "        if query[0] == query_id:\n",
    "            ordered_results = tfidf_engine.get_results(query[1])\n",
    "            break\n",
    "    for i, result in enumerate(ordered_results):\n",
    "        if result['_id'] in relevant_docs:\n",
    "            return 1 / (i+1)\n",
    "    \n",
    "    return 0\n",
    "def calculate_MAP(query_id, tfidf_engine, dataset):\n",
    "    relevant_docs = [qrel.doc_id for qrel in dataset.qrels_iter() if qrel.query_id == query_id]\n",
    "    \n",
    "    ordered_results = []\n",
    "    for query in dataset.queries_iter():\n",
    "        if query.query_id == query_id:\n",
    "            ordered_results = tfidf_engine.get_results(query.text)\n",
    "            break\n",
    "\n",
    "    pk_sum = 0\n",
    "    total_relevant = 0\n",
    "    for i in range(1, 11):\n",
    "        relevant_ret = 0\n",
    "        for j in range(i):\n",
    "            if j < len(ordered_results) and ordered_results[j]['_id'] in relevant_docs:\n",
    "                relevant_ret += 1\n",
    "        p_at_k = (relevant_ret / i) * (1 if i-1 < len(ordered_results) and ordered_results[i-1]['_id'] in relevant_docs else 0)\n",
    "        pk_sum += p_at_k\n",
    "        if i-1 < len(ordered_results) and ordered_results[i-1]['_id'] in relevant_docs:\n",
    "            total_relevant += 1\n",
    "\n",
    "    return 0 if total_relevant == 0 else pk_sum / total_relevant\n",
    "\n",
    "# # Initialize TextPreprocessor\n",
    "text_preprocessor = TextPreprocessor()\n",
    "\n",
    "# # Load documents\n",
    "dataset = ir_datasets.load('antique/train')\n",
    "# documents = [{'id': doc.doc_id, 'text': doc.text} for doc in dataset.docs_iter()]\n",
    "\n",
    "# # Initialize TfidfEngine with the TextPreprocessor\n",
    "# tfidf_engine = TfidfEngine(text_preprocessor)\n",
    "# # Initialize the WordEmbeddingTrainer\n",
    "# # embedding_trainer = WordEmbeddingTrainer(text_preprocessor)\n",
    "\n",
    "\n",
    "\n",
    "# # Load the trained model\n",
    "# tfidf_engine.load_model()\n",
    "\n",
    "# # Calculate MAP for all queries in the dataset\n",
    "# dataset = ir_datasets.load(\"antique/train\")\n",
    "\n",
    "# queries_ids = {qrel.query_id for qrel in dataset.queries_iter()}\n",
    "# map_sum = 0\n",
    "\n",
    "# for query_id in queries_ids:\n",
    "#     map_sum += calculate_MAP(query_id, tfidf_engine, dataset)\n",
    "\n",
    "# mean_average_precision = map_sum / len(queries_ids)\n",
    "# print(f\"Mean Average Precision (MAP): {mean_average_precision}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Average Precision (MAP) for Word Embedding: 0.28236264446772885\n"
     ]
    }
   ],
   "source": [
    "class WordEmbeddingEngine:\n",
    "    def __init__(self, vector_size, sg, workers, epochs, text_processor, text_tokenizer):\n",
    "        self.vector_size = vector_size\n",
    "        self.sg = sg\n",
    "        self.workers = workers\n",
    "        self.epochs = epochs\n",
    "        self.text_processor = text_processor\n",
    "        self.text_tokenizer = text_tokenizer\n",
    "        self.word_embedding_model = None\n",
    "        self.documents_vectors = None\n",
    "        self.document_id_mapping = {}\n",
    "\n",
    "    def init_sentences(self, documents):\n",
    "        sentences = []\n",
    "        for doc_id, document in documents.items():\n",
    "            sentences.append(self.text_tokenizer(self.text_processor.preprocess(document)))\n",
    "            self.document_id_mapping[doc_id] = document\n",
    "        return sentences\n",
    "\n",
    "    def train_model(self, documents):\n",
    "        sentences = self.init_sentences(documents)\n",
    "        model = Word2Vec(sentences,\n",
    "                         vector_size=self.vector_size,\n",
    "                         sg=self.sg,\n",
    "                         workers=self.workers,\n",
    "                         epochs=self.epochs)\n",
    "\n",
    "        self.word_embedding_model = model\n",
    "        self.documents_vectors = self.vectorize_documents(sentences)\n",
    "        self.save_model()\n",
    "\n",
    "    def vectorize_documents(self, sentences):\n",
    "        documents_vectors = []\n",
    "        for sentence in sentences:\n",
    "            zero_vector = np.zeros(self.vector_size)\n",
    "            vectors = []\n",
    "            for token in sentence:\n",
    "                if token in self.word_embedding_model.wv:\n",
    "                    try:\n",
    "                        vectors.append(self.word_embedding_model.wv[token])\n",
    "                    except KeyError:\n",
    "                        vectors.append(np.random(self.vector_size))\n",
    "            if vectors:\n",
    "                vectors = np.asarray(vectors)\n",
    "                avg_vec = vectors.mean(axis=0)\n",
    "                documents_vectors.append(avg_vec)\n",
    "            else:\n",
    "                documents_vectors.append(zero_vector)\n",
    "        return documents_vectors\n",
    "\n",
    "    def save_model(self):\n",
    "        with open('word_embedding_model.pickle', 'wb') as f_model:\n",
    "            pickle.dump(self.word_embedding_model, f_model)\n",
    "        with open('document_vectors.pickle', 'wb') as f_vectors:\n",
    "            pickle.dump(self.documents_vectors, f_vectors)\n",
    "        with open('document_id_mapping_vector.pickle', 'wb') as f_mapping:\n",
    "            pickle.dump(self.document_id_mapping, f_mapping)\n",
    "\n",
    "    def load_model(self):\n",
    "        with open('word_embedding_model.pickle', 'rb') as f_model:\n",
    "            self.word_embedding_model = pickle.load(f_model)\n",
    "        with open('document_vectors.pickle', 'rb') as f_vectors:\n",
    "            self.documents_vectors = pickle.load(f_vectors)\n",
    "        with open('document_id_mapping_vector.pickle', 'rb') as f_mapping:\n",
    "            self.document_id_mapping = pickle.load(f_mapping)\n",
    "\n",
    "    def get_query_vector(self, query_text):\n",
    "        preprocessed_query = self.text_processor.preprocess(query_text)\n",
    "        tokens = self.text_tokenizer(preprocessed_query)\n",
    "        vectors = [self.word_embedding_model.wv[token] for token in tokens if token in self.word_embedding_model.wv]\n",
    "        if vectors:\n",
    "            vectors = np.asarray(vectors)\n",
    "            avg_vec = vectors.mean(axis=0)\n",
    "            return avg_vec\n",
    "        else:\n",
    "            return np.zeros(self.vector_size)\n",
    "\n",
    "    def get_results(self, query_text):\n",
    "        query_vector = self.get_query_vector(query_text)\n",
    "        similarities = cosine_similarity([query_vector], self.documents_vectors).flatten()\n",
    "        ranked_indices = np.argsort(-similarities)\n",
    "        result_ids = []\n",
    "        for idx in ranked_indices[:10]:  # Top 10 results\n",
    "            if similarities[idx] >= 0.35:\n",
    "                result_ids.append(list(self.document_id_mapping.keys())[idx])\n",
    "        unordered_results = [{'_id': doc_id, 'text': self.document_id_mapping[doc_id]} for doc_id in result_ids]\n",
    "        return unordered_results\n",
    "    \n",
    "def calculate_MAP(query_id, engine, dataset):\n",
    "    relevant_docs = [qrel.doc_id for qrel in dataset.qrels_iter() if qrel.query_id == query_id]\n",
    "    \n",
    "    ordered_results = []\n",
    "    for query in dataset.queries_iter():\n",
    "        if query.query_id == query_id:\n",
    "            ordered_results = engine.get_results(query.text)\n",
    "            break\n",
    "\n",
    "    pk_sum = 0\n",
    "    total_relevant = 0\n",
    "    for i in range(1, 11):\n",
    "        relevant_ret = 0\n",
    "        for j in range(i):\n",
    "            if j < len(ordered_results) and ordered_results[j]['_id'] in relevant_docs:\n",
    "                relevant_ret += 1\n",
    "        p_at_k = (relevant_ret / i) * (1 if i-1 < len(ordered_results) and ordered_results[i-1]['_id'] in relevant_docs else 0)\n",
    "        pk_sum += p_at_k\n",
    "        if i-1 < len(ordered_results) and ordered_results[i-1]['_id'] in relevant_docs:\n",
    "            total_relevant += 1\n",
    "\n",
    "    return 0 if total_relevant == 0 else pk_sum / total_relevant\n",
    "\n",
    "# Load documents\n",
    "dataset = ir_datasets.load('antique/train')\n",
    "documents = {doc.doc_id: doc.text for doc in dataset.docs_iter()}\n",
    "\n",
    "# Initialize WordEmbeddingEngine\n",
    "word_embedding_engine = WordEmbeddingEngine(\n",
    "    vector_size=500, sg=1, workers=4, epochs=35,\n",
    "    text_processor=text_preprocessor,\n",
    "    text_tokenizer=tokenize.word_tokenize\n",
    ")\n",
    "\n",
    "# word_embedding_engine.train_model(documents)\n",
    "# print('Train model done!')\n",
    "\n",
    "# Load the trained Word Embedding model\n",
    "word_embedding_engine.load_model()\n",
    "\n",
    "# Calculate MAP for Word Embedding Engine\n",
    "map_sum_word_embedding = 0\n",
    "queries_ids = {qrel.query_id for qrel in dataset.queries_iter()}\n",
    "for query_id in queries_ids:\n",
    "\n",
    "    map_sum_word_embedding += calculate_MAP(query_id, word_embedding_engine, dataset)\n",
    "\n",
    "mean_average_precision_word_embedding = map_sum_word_embedding / len(queries_ids)\n",
    "print(f\"Mean Average Precision (MAP) for Word Embedding: {mean_average_precision_word_embedding}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
