{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "import pickle\n",
    "from typing import List\n",
    "import numpy as np\n",
    "from nltk import tokenize, pos_tag, download\n",
    "from nltk.corpus import stopwords, wordnet\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import ir_datasets\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import pandas as pd\n",
    "from typing import Callable\n",
    "import gensim\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.models.callbacks import CallbackAny2Vec\n",
    "import logging\n",
    "\n",
    "\n",
    "class LemmatizerWithPOSTagger(WordNetLemmatizer):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def _get_wordnet_pos(self, tag: str) -> str:\n",
    "        if tag.startswith('J'):\n",
    "            return wordnet.ADJ\n",
    "        elif tag.startswith('V'):\n",
    "            return wordnet.VERB\n",
    "        elif tag.startswith('N'):\n",
    "            return wordnet.NOUN\n",
    "        elif tag.startswith('R'):\n",
    "            return wordnet.ADV\n",
    "        else:\n",
    "            return wordnet.NOUN\n",
    "\n",
    "    def lemmatize(self, word: str, pos: str = \"n\") -> str:\n",
    "        return super().lemmatize(word, self._get_wordnet_pos(pos))\n",
    "\n",
    "class TextPreprocessor():\n",
    "\n",
    "    def __init__(self, tokenizer: Callable = None) -> None:\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "        if self.tokenizer is None:\n",
    "            self.tokenizer = tokenize.word_tokenize\n",
    "\n",
    "        self.stopwords_tokens = stopwords.words('english')\n",
    "        self.stemmer = PorterStemmer()\n",
    "        self.lemmatizer = LemmatizerWithPOSTagger()\n",
    "\n",
    "    def tokenize(self, text: str)-> List[str]:\n",
    "        tokens =self.tokenizer(text)\n",
    "        return tokens\n",
    "    \n",
    "    def to_lower(self, tokens: List[str]) -> List[str]:\n",
    "        lower_tokens = []\n",
    "        for token in tokens:\n",
    "            lower_token = str(np.char.lower(token))\n",
    "            lower_tokens.append(lower_token)\n",
    "        return lower_tokens\n",
    "\n",
    "    \n",
    "    def remove_markers(self, tokens: List[str]) -> List[str]:\n",
    "        new_tokens = []\n",
    "        for token in tokens:\n",
    "            new_tokens.append(re.sub(r'\\u00AE', '', token))\n",
    "        return new_tokens\n",
    "\n",
    "    def remove_punctuation(self, tokens: List[str]) ->  List[str]:\n",
    "        new_tokens = []\n",
    "        for token in tokens:\n",
    "            new_tokens.append(token.translate(str.maketrans('', '', string.punctuation)))\n",
    "        return new_tokens\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def rplace_under_score_with_space(self, tokens: List[str]) -> List[str]:\n",
    "        new_tokens = []\n",
    "        for token in tokens:\n",
    "            new_tokens.append(re.sub(r'_', ' ', token))\n",
    "        return new_tokens\n",
    "\n",
    "    def remove_stop_words(self,tokens: List[str]) -> List[str]:\n",
    "        new_tokens = []\n",
    "        for token in tokens:\n",
    "            if token not in self.stopwords_tokens and len(token) > 1:\n",
    "                new_tokens.append(token)\n",
    "        return new_tokens\n",
    "\n",
    "    def remove_apostrophe(self, tokens: List[str]) -> List[str]:\n",
    "        new_tokens = []\n",
    "        for token in tokens:\n",
    "            new_tokens.append(str(np.char.replace(token, \"'\", \" \")))\n",
    "        return new_tokens\n",
    "\n",
    "    def stemming(self, tokens: List[str]) -> List[str]:\n",
    "        new_tokens = []\n",
    "        for token in tokens:\n",
    "            new_tokens.append(self.stemmer.stem(token))\n",
    "        return new_tokens\n",
    "    \n",
    "    \n",
    "    def normalize_appreviations(self, tokens: List[str]) -> List[str]:\n",
    "        new_tokens = []\n",
    "        resolved_terms = {}\n",
    "        for token in tokens:\n",
    "\n",
    "            if len(token) >= 2:\n",
    "                synsets = wordnet.synsets(token)\n",
    "                if synsets:\n",
    "                    resolved_term = synsets[0].lemmas()[0].name()\n",
    "                    resolved_terms[token] = resolved_term\n",
    "\n",
    "        for abbreviation, resolved_term in resolved_terms.items():\n",
    "            for i in range(len(tokens)):\n",
    "                if tokens[i] == abbreviation:\n",
    "                    tokens[i] = resolved_term\n",
    "                    break\n",
    "\n",
    "        return tokens\n",
    "    \n",
    "    def lemmatizing(self, tokens: List[str]) -> List[str]:\n",
    "        tagged_tokens = pos_tag(tokens)\n",
    "        lemmatized_tokens = [self.lemmatizer.lemmatize(token, pos) for token, pos in tagged_tokens]\n",
    "        return lemmatized_tokens\n",
    "\n",
    "\n",
    "    def preprocess(self, text: str) -> str:\n",
    "        operations = [\n",
    "            self.to_lower,\n",
    "            self.remove_punctuation,\n",
    "            self.remove_apostrophe,\n",
    "            self.remove_stop_words,\n",
    "            self.remove_markers,\n",
    "            self.stemming,\n",
    "            self.lemmatizing,\n",
    "            self.normalize_appreviations, \n",
    "            self.to_lower,\n",
    "            self.rplace_under_score_with_space\n",
    "        ]\n",
    "        text_tokens=self.tokenize(text)\n",
    "        for op in operations:\n",
    "              text_tokens=op(text_tokens)\n",
    "    \n",
    "        new_text=\"\"\n",
    "        new_text = ' '.join(text_tokens)\n",
    "            \n",
    "        return new_text\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # Initialize TextPreprocessor\n",
    "text_preprocessor = TextPreprocessor()\n",
    "\n",
    "# # Load documents\n",
    "dataset = ir_datasets.load('wikir/en1k/training')\n",
    "# documents = [{'id': doc.doc_id, 'text': doc.text} for doc in dataset.docs_iter()]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train model done!\n",
      "Mean Average Precision (MAP) for Word Embedding: 0.49152639685837435\n"
     ]
    }
   ],
   "source": [
    "class WordEmbeddingEngine:\n",
    "    def __init__(self, vector_size, sg, workers, epochs, text_processor, text_tokenizer):\n",
    "        self.vector_size = vector_size\n",
    "        self.sg = sg\n",
    "        self.workers = workers\n",
    "        self.epochs = epochs\n",
    "        self.text_processor = text_processor\n",
    "        self.text_tokenizer = text_tokenizer\n",
    "        self.word_embedding_model = None\n",
    "        self.documents_vectors = None\n",
    "        self.document_id_mapping = {}\n",
    "\n",
    "    def init_sentences(self, documents):\n",
    "        sentences = []\n",
    "        for doc_id, document in documents.items():\n",
    "            sentences.append(self.text_tokenizer(self.text_processor.preprocess(document)))\n",
    "            self.document_id_mapping[doc_id] = document\n",
    "        return sentences\n",
    "\n",
    "    def train_model(self, documents):\n",
    "        sentences = self.init_sentences(documents)\n",
    "        model = Word2Vec(sentences,\n",
    "                         vector_size=self.vector_size,\n",
    "                         sg=self.sg,\n",
    "                         workers=self.workers,\n",
    "                         epochs=self.epochs)\n",
    "\n",
    "        self.word_embedding_model = model\n",
    "        self.documents_vectors = self.vectorize_documents(sentences)\n",
    "        self.save_model()\n",
    "\n",
    "    def vectorize_documents(self, sentences):\n",
    "        documents_vectors = []\n",
    "        for sentence in sentences:\n",
    "            zero_vector = np.zeros(self.vector_size)\n",
    "            vectors = []\n",
    "            for token in sentence:\n",
    "                if token in self.word_embedding_model.wv:\n",
    "                    try:\n",
    "                        vectors.append(self.word_embedding_model.wv[token])\n",
    "                    except KeyError:\n",
    "                        vectors.append(np.random(self.vector_size))\n",
    "            if vectors:\n",
    "                vectors = np.asarray(vectors)\n",
    "                avg_vec = vectors.mean(axis=0)\n",
    "                documents_vectors.append(avg_vec)\n",
    "            else:\n",
    "                documents_vectors.append(zero_vector)\n",
    "        return documents_vectors\n",
    "\n",
    "    def save_model(self):\n",
    "        with open('word_embedding_model_wiki.pickle', 'wb') as f_model:\n",
    "            pickle.dump(self.word_embedding_model, f_model)\n",
    "        with open('document_vectors_wiki.pickle', 'wb') as f_vectors:\n",
    "            pickle.dump(self.documents_vectors, f_vectors)\n",
    "        with open('document_id_mapping_vector_wiki.pickle', 'wb') as f_mapping:\n",
    "            pickle.dump(self.document_id_mapping, f_mapping)\n",
    "\n",
    "    def load_model(self):\n",
    "        with open('word_embedding_model_wiki.pickle', 'rb') as f_model:\n",
    "            self.word_embedding_model = pickle.load(f_model)\n",
    "        with open('document_vectors_wiki.pickle', 'rb') as f_vectors:\n",
    "            self.documents_vectors = pickle.load(f_vectors)\n",
    "        with open('document_id_mapping_vector_wiki.pickle', 'rb') as f_mapping:\n",
    "            self.document_id_mapping = pickle.load(f_mapping)\n",
    "\n",
    "    def get_query_vector(self, query_text):\n",
    "        preprocessed_query = self.text_processor.preprocess(query_text)\n",
    "        tokens = self.text_tokenizer(preprocessed_query)\n",
    "        vectors = [self.word_embedding_model.wv[token] for token in tokens if token in self.word_embedding_model.wv]\n",
    "        if vectors:\n",
    "            vectors = np.asarray(vectors)\n",
    "            avg_vec = vectors.mean(axis=0)\n",
    "            return avg_vec\n",
    "        else:\n",
    "            return np.zeros(self.vector_size)\n",
    "\n",
    "    def get_results(self, query_text):\n",
    "        query_vector = self.get_query_vector(query_text)\n",
    "        similarities = cosine_similarity([query_vector], self.documents_vectors).flatten()\n",
    "        ranked_indices = np.argsort(-similarities)\n",
    "        result_ids = []\n",
    "        for idx in ranked_indices[:10]:  # Top 10 results\n",
    "            if similarities[idx] >= 0.35:\n",
    "                result_ids.append(list(self.document_id_mapping.keys())[idx])\n",
    "        unordered_results = [{'_id': doc_id, 'text': self.document_id_mapping[doc_id]} for doc_id in result_ids]\n",
    "        return unordered_results\n",
    "    \n",
    "def calculate_MAP(query_id, engine, dataset):\n",
    "    relevant_docs = [qrel.doc_id for qrel in dataset.qrels_iter() if qrel.query_id == query_id]\n",
    "    \n",
    "    ordered_results = []\n",
    "    for query in dataset.queries_iter():\n",
    "        if query.query_id == query_id:\n",
    "            ordered_results = engine.get_results(query.text)\n",
    "            break\n",
    "\n",
    "    pk_sum = 0\n",
    "    total_relevant = 0\n",
    "    for i in range(1, 11):\n",
    "        relevant_ret = 0\n",
    "        for j in range(i):\n",
    "            if j < len(ordered_results) and ordered_results[j]['_id'] in relevant_docs:\n",
    "                relevant_ret += 1\n",
    "        p_at_k = (relevant_ret / i) * (1 if i-1 < len(ordered_results) and ordered_results[i-1]['_id'] in relevant_docs else 0)\n",
    "        pk_sum += p_at_k\n",
    "        if i-1 < len(ordered_results) and ordered_results[i-1]['_id'] in relevant_docs:\n",
    "            total_relevant += 1\n",
    "\n",
    "    return 0 if total_relevant == 0 else pk_sum / total_relevant\n",
    "\n",
    "# Load documents\n",
    "dataset = ir_datasets.load('wikir/en1k/training')\n",
    "documents = {doc.doc_id: doc.text for doc in dataset.docs_iter()}\n",
    "\n",
    "# Initialize WordEmbeddingEngine\n",
    "word_embedding_engine = WordEmbeddingEngine(\n",
    "    vector_size=500, sg=1, workers=4, epochs=35,\n",
    "    text_processor=text_preprocessor,\n",
    "    text_tokenizer=tokenize.word_tokenize\n",
    ")\n",
    "\n",
    "# Train the Word Embedding model (uncomment to train and save the model)\n",
    "word_embedding_engine.train_model(documents)\n",
    "print('Train model done!')\n",
    "\n",
    "# Load the trained Word Embedding model\n",
    "word_embedding_engine.load_model()\n",
    "\n",
    "# Calculate MAP for Word Embedding Engine\n",
    "map_sum_word_embedding = 0\n",
    "queries_ids = {qrel.query_id for qrel in dataset.queries_iter()}\n",
    "for query_id in queries_ids:\n",
    "\n",
    "    map_sum_word_embedding += calculate_MAP(query_id, word_embedding_engine, dataset)\n",
    "\n",
    "mean_average_precision_word_embedding = map_sum_word_embedding / len(queries_ids)\n",
    "print(f\"Mean Average Precision (MAP) for Word Embedding: {mean_average_precision_word_embedding}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
