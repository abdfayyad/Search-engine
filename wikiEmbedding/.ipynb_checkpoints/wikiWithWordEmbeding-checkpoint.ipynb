{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "import pickle\n",
    "from typing import List\n",
    "import numpy as np\n",
    "from nltk import tokenize, pos_tag, download\n",
    "from nltk.corpus import stopwords, wordnet\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import ir_datasets\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import pandas as pd\n",
    "from typing import Callable\n",
    "import gensim\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.models.callbacks import CallbackAny2Vec\n",
    "import logging\n",
    "\n",
    "\n",
    "class LemmatizerWithPOSTagger(WordNetLemmatizer):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def _get_wordnet_pos(self, tag: str) -> str:\n",
    "        if tag.startswith('J'):\n",
    "            return wordnet.ADJ\n",
    "        elif tag.startswith('V'):\n",
    "            return wordnet.VERB\n",
    "        elif tag.startswith('N'):\n",
    "            return wordnet.NOUN\n",
    "        elif tag.startswith('R'):\n",
    "            return wordnet.ADV\n",
    "        else:\n",
    "            return wordnet.NOUN\n",
    "\n",
    "    def lemmatize(self, word: str, pos: str = \"n\") -> str:\n",
    "        return super().lemmatize(word, self._get_wordnet_pos(pos))\n",
    "\n",
    "class TextPreprocessor():\n",
    "\n",
    "    def __init__(self, tokenizer: Callable = None) -> None:\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "        if self.tokenizer is None:\n",
    "            self.tokenizer = tokenize.word_tokenize\n",
    "\n",
    "        self.stopwords_tokens = stopwords.words('english')\n",
    "        self.stemmer = PorterStemmer()\n",
    "        self.lemmatizer = LemmatizerWithPOSTagger()\n",
    "\n",
    "    def tokenize(self, text: str)-> List[str]:\n",
    "        tokens =self.tokenizer(text)\n",
    "        return tokens\n",
    "    \n",
    "    def to_lower(self, tokens: List[str]) -> List[str]:\n",
    "        lower_tokens = []\n",
    "        for token in tokens:\n",
    "            lower_token = str(np.char.lower(token))\n",
    "            lower_tokens.append(lower_token)\n",
    "        return lower_tokens\n",
    "\n",
    "    \n",
    "    def remove_markers(self, tokens: List[str]) -> List[str]:\n",
    "        new_tokens = []\n",
    "        for token in tokens:\n",
    "            new_tokens.append(re.sub(r'\\u00AE', '', token))\n",
    "        return new_tokens\n",
    "\n",
    "    def remove_punctuation(self, tokens: List[str]) ->  List[str]:\n",
    "        new_tokens = []\n",
    "        for token in tokens:\n",
    "            new_tokens.append(token.translate(str.maketrans('', '', string.punctuation)))\n",
    "        return new_tokens\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def rplace_under_score_with_space(self, tokens: List[str]) -> List[str]:\n",
    "        new_tokens = []\n",
    "        for token in tokens:\n",
    "            new_tokens.append(re.sub(r'_', ' ', token))\n",
    "        return new_tokens\n",
    "\n",
    "    def remove_stop_words(self,tokens: List[str]) -> List[str]:\n",
    "        new_tokens = []\n",
    "        for token in tokens:\n",
    "            if token not in self.stopwords_tokens and len(token) > 1:\n",
    "                new_tokens.append(token)\n",
    "        return new_tokens\n",
    "\n",
    "    def remove_apostrophe(self, tokens: List[str]) -> List[str]:\n",
    "        new_tokens = []\n",
    "        for token in tokens:\n",
    "            new_tokens.append(str(np.char.replace(token, \"'\", \" \")))\n",
    "        return new_tokens\n",
    "\n",
    "    def stemming(self, tokens: List[str]) -> List[str]:\n",
    "        new_tokens = []\n",
    "        for token in tokens:\n",
    "            new_tokens.append(self.stemmer.stem(token))\n",
    "        return new_tokens\n",
    "    \n",
    "    \n",
    "    def normalize_appreviations(self, tokens: List[str]) -> List[str]:\n",
    "        new_tokens = []\n",
    "        resolved_terms = {}\n",
    "        for token in tokens:\n",
    "\n",
    "            if len(token) >= 2:\n",
    "                synsets = wordnet.synsets(token)\n",
    "                if synsets:\n",
    "                    resolved_term = synsets[0].lemmas()[0].name()\n",
    "                    resolved_terms[token] = resolved_term\n",
    "\n",
    "        for abbreviation, resolved_term in resolved_terms.items():\n",
    "            for i in range(len(tokens)):\n",
    "                if tokens[i] == abbreviation:\n",
    "                    tokens[i] = resolved_term\n",
    "                    break\n",
    "\n",
    "        return tokens\n",
    "    \n",
    "    def lemmatizing(self, tokens: List[str]) -> List[str]:\n",
    "        tagged_tokens = pos_tag(tokens)\n",
    "        lemmatized_tokens = [self.lemmatizer.lemmatize(token, pos) for token, pos in tagged_tokens]\n",
    "        return lemmatized_tokens\n",
    "\n",
    "\n",
    "    def preprocess(self, text: str) -> str:\n",
    "        operations = [\n",
    "            self.to_lower,\n",
    "            self.remove_punctuation,\n",
    "            self.remove_apostrophe,\n",
    "            self.remove_stop_words,\n",
    "            self.remove_markers,\n",
    "            self.stemming,\n",
    "            self.lemmatizing,\n",
    "            self.normalize_appreviations, \n",
    "            self.to_lower,\n",
    "            self.rplace_under_score_with_space\n",
    "        ]\n",
    "        text_tokens=self.tokenize(text)\n",
    "        for op in operations:\n",
    "              text_tokens=op(text_tokens)\n",
    "    \n",
    "        new_text=\"\"\n",
    "        new_text = ' '.join(text_tokens)\n",
    "            \n",
    "        return new_text\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # Initialize TextPreprocessor\n",
    "text_preprocessor = TextPreprocessor()\n",
    "\n",
    "# # Load documents\n",
    "dataset = ir_datasets.load('wikir/en1k/training')\n",
    "# documents = [{'id': doc.doc_id, 'text': doc.text} for doc in dataset.docs_iter()]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 124\u001b[0m\n\u001b[1;32m    117\u001b[0m word_embedding_engine \u001b[38;5;241m=\u001b[39m WordEmbeddingEngine(\n\u001b[1;32m    118\u001b[0m     vector_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m500\u001b[39m, sg\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, workers\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m4\u001b[39m, epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m35\u001b[39m,\n\u001b[1;32m    119\u001b[0m     text_processor\u001b[38;5;241m=\u001b[39mtext_preprocessor,\n\u001b[1;32m    120\u001b[0m     text_tokenizer\u001b[38;5;241m=\u001b[39mtokenize\u001b[38;5;241m.\u001b[39mword_tokenize\n\u001b[1;32m    121\u001b[0m )\n\u001b[1;32m    123\u001b[0m \u001b[38;5;66;03m# Train the Word Embedding model (uncomment to train and save the model)\u001b[39;00m\n\u001b[0;32m--> 124\u001b[0m word_embedding_engine\u001b[38;5;241m.\u001b[39mtrain_model(documents)\n\u001b[1;32m    125\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTrain model done!\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "Cell \u001b[0;32mIn[3], line 22\u001b[0m, in \u001b[0;36mWordEmbeddingEngine.train_model\u001b[0;34m(self, documents)\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtrain_model\u001b[39m(\u001b[38;5;28mself\u001b[39m, documents):\n\u001b[1;32m     21\u001b[0m     sentences \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minit_sentences(documents)\n\u001b[0;32m---> 22\u001b[0m     model \u001b[38;5;241m=\u001b[39m Word2Vec(sentences,\n\u001b[1;32m     23\u001b[0m                      vector_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvector_size,\n\u001b[1;32m     24\u001b[0m                      sg\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msg,\n\u001b[1;32m     25\u001b[0m                      workers\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mworkers,\n\u001b[1;32m     26\u001b[0m                      epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mepochs)\n\u001b[1;32m     28\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mword_embedding_model \u001b[38;5;241m=\u001b[39m model\n\u001b[1;32m     29\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdocuments_vectors \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvectorize_documents(sentences)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/gensim/models/word2vec.py:430\u001b[0m, in \u001b[0;36mWord2Vec.__init__\u001b[0;34m(self, sentences, corpus_file, vector_size, alpha, window, min_count, max_vocab_size, sample, seed, workers, min_alpha, sg, hs, negative, ns_exponent, cbow_mean, hashfxn, epochs, null_word, trim_rule, sorted_vocab, batch_words, compute_loss, callbacks, comment, max_final_vocab, shrink_windows)\u001b[0m\n\u001b[1;32m    428\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_corpus_sanity(corpus_iterable\u001b[38;5;241m=\u001b[39mcorpus_iterable, corpus_file\u001b[38;5;241m=\u001b[39mcorpus_file, passes\u001b[38;5;241m=\u001b[39m(epochs \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m))\n\u001b[1;32m    429\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuild_vocab(corpus_iterable\u001b[38;5;241m=\u001b[39mcorpus_iterable, corpus_file\u001b[38;5;241m=\u001b[39mcorpus_file, trim_rule\u001b[38;5;241m=\u001b[39mtrim_rule)\n\u001b[0;32m--> 430\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain(\n\u001b[1;32m    431\u001b[0m         corpus_iterable\u001b[38;5;241m=\u001b[39mcorpus_iterable, corpus_file\u001b[38;5;241m=\u001b[39mcorpus_file, total_examples\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcorpus_count,\n\u001b[1;32m    432\u001b[0m         total_words\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcorpus_total_words, epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mepochs, start_alpha\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39malpha,\n\u001b[1;32m    433\u001b[0m         end_alpha\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmin_alpha, compute_loss\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompute_loss, callbacks\u001b[38;5;241m=\u001b[39mcallbacks)\n\u001b[1;32m    434\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    435\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m trim_rule \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/gensim/models/word2vec.py:1073\u001b[0m, in \u001b[0;36mWord2Vec.train\u001b[0;34m(self, corpus_iterable, corpus_file, total_examples, total_words, epochs, start_alpha, end_alpha, word_count, queue_factor, report_delay, compute_loss, callbacks, **kwargs)\u001b[0m\n\u001b[1;32m   1070\u001b[0m     callback\u001b[38;5;241m.\u001b[39mon_epoch_begin(\u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m   1072\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m corpus_iterable \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1073\u001b[0m     trained_word_count_epoch, raw_word_count_epoch, job_tally_epoch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_train_epoch(\n\u001b[1;32m   1074\u001b[0m         corpus_iterable, cur_epoch\u001b[38;5;241m=\u001b[39mcur_epoch, total_examples\u001b[38;5;241m=\u001b[39mtotal_examples,\n\u001b[1;32m   1075\u001b[0m         total_words\u001b[38;5;241m=\u001b[39mtotal_words, queue_factor\u001b[38;5;241m=\u001b[39mqueue_factor, report_delay\u001b[38;5;241m=\u001b[39mreport_delay,\n\u001b[1;32m   1076\u001b[0m         callbacks\u001b[38;5;241m=\u001b[39mcallbacks, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1077\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1078\u001b[0m     trained_word_count_epoch, raw_word_count_epoch, job_tally_epoch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_train_epoch_corpusfile(\n\u001b[1;32m   1079\u001b[0m         corpus_file, cur_epoch\u001b[38;5;241m=\u001b[39mcur_epoch, total_examples\u001b[38;5;241m=\u001b[39mtotal_examples, total_words\u001b[38;5;241m=\u001b[39mtotal_words,\n\u001b[1;32m   1080\u001b[0m         callbacks\u001b[38;5;241m=\u001b[39mcallbacks, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/gensim/models/word2vec.py:1434\u001b[0m, in \u001b[0;36mWord2Vec._train_epoch\u001b[0;34m(self, data_iterable, cur_epoch, total_examples, total_words, queue_factor, report_delay, callbacks)\u001b[0m\n\u001b[1;32m   1431\u001b[0m     thread\u001b[38;5;241m.\u001b[39mdaemon \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m  \u001b[38;5;66;03m# make interrupting the process with ctrl+c easier\u001b[39;00m\n\u001b[1;32m   1432\u001b[0m     thread\u001b[38;5;241m.\u001b[39mstart()\n\u001b[0;32m-> 1434\u001b[0m trained_word_count, raw_word_count, job_tally \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_log_epoch_progress(\n\u001b[1;32m   1435\u001b[0m     progress_queue, job_queue, cur_epoch\u001b[38;5;241m=\u001b[39mcur_epoch, total_examples\u001b[38;5;241m=\u001b[39mtotal_examples,\n\u001b[1;32m   1436\u001b[0m     total_words\u001b[38;5;241m=\u001b[39mtotal_words, report_delay\u001b[38;5;241m=\u001b[39mreport_delay, is_corpus_file_mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m   1437\u001b[0m )\n\u001b[1;32m   1439\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m trained_word_count, raw_word_count, job_tally\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/gensim/models/word2vec.py:1289\u001b[0m, in \u001b[0;36mWord2Vec._log_epoch_progress\u001b[0;34m(self, progress_queue, job_queue, cur_epoch, total_examples, total_words, report_delay, is_corpus_file_mode)\u001b[0m\n\u001b[1;32m   1286\u001b[0m unfinished_worker_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mworkers\n\u001b[1;32m   1288\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m unfinished_worker_count \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m-> 1289\u001b[0m     report \u001b[38;5;241m=\u001b[39m progress_queue\u001b[38;5;241m.\u001b[39mget()  \u001b[38;5;66;03m# blocks if workers too slow\u001b[39;00m\n\u001b[1;32m   1290\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m report \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:  \u001b[38;5;66;03m# a thread reporting that it finished\u001b[39;00m\n\u001b[1;32m   1291\u001b[0m         unfinished_worker_count \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/queue.py:171\u001b[0m, in \u001b[0;36mQueue.get\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m    169\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    170\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_qsize():\n\u001b[0;32m--> 171\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnot_empty\u001b[38;5;241m.\u001b[39mwait()\n\u001b[1;32m    172\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m timeout \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    173\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtimeout\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m must be a non-negative number\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/threading.py:327\u001b[0m, in \u001b[0;36mCondition.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:    \u001b[38;5;66;03m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[39;00m\n\u001b[1;32m    326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 327\u001b[0m         waiter\u001b[38;5;241m.\u001b[39macquire()\n\u001b[1;32m    328\u001b[0m         gotit \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    329\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "class WordEmbeddingEngine:\n",
    "    def __init__(self, vector_size, sg, workers, epochs, text_processor, text_tokenizer):\n",
    "        self.vector_size = vector_size\n",
    "        self.sg = sg\n",
    "        self.workers = workers\n",
    "        self.epochs = epochs\n",
    "        self.text_processor = text_processor\n",
    "        self.text_tokenizer = text_tokenizer\n",
    "        self.word_embedding_model = None\n",
    "        self.documents_vectors = None\n",
    "        self.document_id_mapping = {}\n",
    "\n",
    "    def init_sentences(self, documents):\n",
    "        sentences = []\n",
    "        for doc_id, document in documents.items():\n",
    "            sentences.append(self.text_tokenizer(self.text_processor.preprocess(document)))\n",
    "            self.document_id_mapping[doc_id] = document\n",
    "        return sentences\n",
    "\n",
    "    def train_model(self, documents):\n",
    "        sentences = self.init_sentences(documents)\n",
    "        model = Word2Vec(sentences,\n",
    "                         vector_size=self.vector_size,\n",
    "                         sg=self.sg,\n",
    "                         workers=self.workers,\n",
    "                         epochs=self.epochs)\n",
    "\n",
    "        self.word_embedding_model = model\n",
    "        self.documents_vectors = self.vectorize_documents(sentences)\n",
    "        self.save_model()\n",
    "\n",
    "    def vectorize_documents(self, sentences):\n",
    "        documents_vectors = []\n",
    "        for sentence in sentences:\n",
    "            zero_vector = np.zeros(self.vector_size)\n",
    "            vectors = []\n",
    "            for token in sentence:\n",
    "                if token in self.word_embedding_model.wv:\n",
    "                    try:\n",
    "                        vectors.append(self.word_embedding_model.wv[token])\n",
    "                    except KeyError:\n",
    "                        vectors.append(np.random(self.vector_size))\n",
    "            if vectors:\n",
    "                vectors = np.asarray(vectors)\n",
    "                avg_vec = vectors.mean(axis=0)\n",
    "                documents_vectors.append(avg_vec)\n",
    "            else:\n",
    "                documents_vectors.append(zero_vector)\n",
    "        return documents_vectors\n",
    "\n",
    "    def save_model(self):\n",
    "        with open('word_embedding_model_wiki.pickle', 'wb') as f_model:\n",
    "            pickle.dump(self.word_embedding_model, f_model)\n",
    "        with open('document_vectors_wiki.pickle', 'wb') as f_vectors:\n",
    "            pickle.dump(self.documents_vectors, f_vectors)\n",
    "        with open('document_id_mapping_vector_wiki.pickle', 'wb') as f_mapping:\n",
    "            pickle.dump(self.document_id_mapping, f_mapping)\n",
    "\n",
    "    def load_model(self):\n",
    "        with open('word_embedding_model_wiki.pickle', 'rb') as f_model:\n",
    "            self.word_embedding_model = pickle.load(f_model)\n",
    "        with open('document_vectors_wiki.pickle', 'rb') as f_vectors:\n",
    "            self.documents_vectors = pickle.load(f_vectors)\n",
    "        with open('document_id_mapping_vector_wiki.pickle', 'rb') as f_mapping:\n",
    "            self.document_id_mapping = pickle.load(f_mapping)\n",
    "\n",
    "    def get_query_vector(self, query_text):\n",
    "        preprocessed_query = self.text_processor.preprocess(query_text)\n",
    "        tokens = self.text_tokenizer(preprocessed_query)\n",
    "        vectors = [self.word_embedding_model.wv[token] for token in tokens if token in self.word_embedding_model.wv]\n",
    "        if vectors:\n",
    "            vectors = np.asarray(vectors)\n",
    "            avg_vec = vectors.mean(axis=0)\n",
    "            return avg_vec\n",
    "        else:\n",
    "            return np.zeros(self.vector_size)\n",
    "\n",
    "    def get_results(self, query_text):\n",
    "        query_vector = self.get_query_vector(query_text)\n",
    "        similarities = cosine_similarity([query_vector], self.documents_vectors).flatten()\n",
    "        ranked_indices = np.argsort(-similarities)\n",
    "        result_ids = []\n",
    "        for idx in ranked_indices[:10]:  # Top 10 results\n",
    "            if similarities[idx] >= 0.35:\n",
    "                result_ids.append(list(self.document_id_mapping.keys())[idx])\n",
    "        unordered_results = [{'_id': doc_id, 'text': self.document_id_mapping[doc_id]} for doc_id in result_ids]\n",
    "        return unordered_results\n",
    "    \n",
    "def calculate_MAP(query_id, engine, dataset):\n",
    "    relevant_docs = [qrel.doc_id for qrel in dataset.qrels_iter() if qrel.query_id == query_id]\n",
    "    \n",
    "    ordered_results = []\n",
    "    for query in dataset.queries_iter():\n",
    "        if query.query_id == query_id:\n",
    "            ordered_results = engine.get_results(query.text)\n",
    "            break\n",
    "\n",
    "    pk_sum = 0\n",
    "    total_relevant = 0\n",
    "    for i in range(1, 11):\n",
    "        relevant_ret = 0\n",
    "        for j in range(i):\n",
    "            if j < len(ordered_results) and ordered_results[j]['_id'] in relevant_docs:\n",
    "                relevant_ret += 1\n",
    "        p_at_k = (relevant_ret / i) * (1 if i-1 < len(ordered_results) and ordered_results[i-1]['_id'] in relevant_docs else 0)\n",
    "        pk_sum += p_at_k\n",
    "        if i-1 < len(ordered_results) and ordered_results[i-1]['_id'] in relevant_docs:\n",
    "            total_relevant += 1\n",
    "\n",
    "    return 0 if total_relevant == 0 else pk_sum / total_relevant\n",
    "\n",
    "# Load documents\n",
    "dataset = ir_datasets.load('wikir/en1k/training')\n",
    "documents = {doc.doc_id: doc.text for doc in dataset.docs_iter()}\n",
    "\n",
    "# Initialize WordEmbeddingEngine\n",
    "word_embedding_engine = WordEmbeddingEngine(\n",
    "    vector_size=500, sg=1, workers=4, epochs=35,\n",
    "    text_processor=text_preprocessor,\n",
    "    text_tokenizer=tokenize.word_tokenize\n",
    ")\n",
    "\n",
    "# Train the Word Embedding model (uncomment to train and save the model)\n",
    "word_embedding_engine.train_model(documents)\n",
    "print('Train model done!')\n",
    "\n",
    "# # Load the trained Word Embedding model\n",
    "# word_embedding_engine.load_model()\n",
    "\n",
    "# # Calculate MAP for Word Embedding Engine\n",
    "# map_sum_word_embedding = 0\n",
    "# queries_ids = {qrel.query_id for qrel in dataset.queries_iter()}\n",
    "# for query_id in queries_ids:\n",
    "\n",
    "#     map_sum_word_embedding += calculate_MAP(query_id, word_embedding_engine, dataset)\n",
    "\n",
    "# mean_average_precision_word_embedding = map_sum_word_embedding / len(queries_ids)\n",
    "# print(f\"Mean Average Precision (MAP) for Word Embedding: {mean_average_precision_word_embedding}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
