{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train model done!\n",
      "Mean Average Precision (MAP) for Word Embedding: 0.715059470269589\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import string\n",
    "import pickle\n",
    "from typing import List\n",
    "import numpy as np\n",
    "from nltk import tokenize, pos_tag, download\n",
    "from nltk.corpus import stopwords, wordnet\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import ir_datasets\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import pandas as pd\n",
    "from typing import Callable\n",
    "import re\n",
    "import string\n",
    "from typing import Callable, List\n",
    "from nltk.corpus import stopwords, wordnet\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk import pos_tag\n",
    "import numpy as np\n",
    "import inflect\n",
    "import contractions\n",
    "import gensim\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.models.callbacks import CallbackAny2Vec\n",
    "import logging\n",
    "\n",
    "class LemmatizerWithPOSTagger(WordNetLemmatizer):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def _get_wordnet_pos(self, tag: str) -> str:\n",
    "        if tag.startswith('J'):\n",
    "            return wordnet.ADJ\n",
    "        elif tag.startswith('V'):\n",
    "            return wordnet.VERB\n",
    "        elif tag.startswith('N'):\n",
    "            return wordnet.NOUN\n",
    "        elif tag.startswith('R'):\n",
    "            return wordnet.ADV\n",
    "        else:\n",
    "            return wordnet.NOUN\n",
    "\n",
    "    def lemmatize(self, word: str, pos: str = \"n\") -> str:\n",
    "        return super().lemmatize(word, self._get_wordnet_pos(pos))\n",
    "\n",
    "class TextPreprocessor():\n",
    "\n",
    "    def __init__(self, tokenizer: Callable = None) -> None:\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "        if self.tokenizer is None:\n",
    "            self.tokenizer = word_tokenize\n",
    "\n",
    "        self.stopwords_tokens = stopwords.words('english')\n",
    "        self.stemmer = PorterStemmer()\n",
    "        self.lemmatizer = LemmatizerWithPOSTagger()\n",
    "        self.inflect_engine = inflect.engine()\n",
    "\n",
    "    def tokenize(self, text: str) -> List[str]:\n",
    "        tokens = self.tokenizer(text)\n",
    "        return tokens\n",
    "    \n",
    "    def to_lower(self, tokens: List[str]) -> List[str]:\n",
    "        return [token.lower() for token in tokens]\n",
    "\n",
    "    def remove_markers(self, tokens: List[str]) -> List[str]:\n",
    "        return [re.sub(r'\\u00AE', '', token) for token in tokens]\n",
    "\n",
    "    def remove_punctuation(self, tokens: List[str]) -> List[str]:\n",
    "        return [token.translate(str.maketrans('', '', string.punctuation)) for token in tokens]\n",
    "\n",
    "    def replace_under_score_with_space(self, tokens: List[str]) -> List[str]:\n",
    "        return [token.replace('_', ' ') for token in tokens]\n",
    "\n",
    "    def remove_stop_words(self, tokens: List[str]) -> List[str]:\n",
    "        return [token for token in tokens if token not in self.stopwords_tokens and len(token) > 1]\n",
    "\n",
    "    def remove_apostrophe(self, tokens: List[str]) -> List[str]:\n",
    "        return [token.replace(\"'\", \" \") for token in tokens]\n",
    "\n",
    "    def stemming(self, tokens: List[str]) -> List[str]:\n",
    "        return [self.stemmer.stem(token) for token in tokens]\n",
    "\n",
    "    def normalize_abbreviations(self, tokens: List[str]) -> List[str]:\n",
    "        new_tokens = []\n",
    "        resolved_terms = {}\n",
    "        for token in tokens:\n",
    "            if len(token) >= 2:\n",
    "                synsets = wordnet.synsets(token)\n",
    "                if synsets:\n",
    "                    resolved_term = synsets[0].lemmas()[0].name()\n",
    "                    resolved_terms[token] = resolved_term\n",
    "\n",
    "        for token in tokens:\n",
    "            if token in resolved_terms:\n",
    "                new_tokens.append(resolved_terms[token])\n",
    "            else:\n",
    "                new_tokens.append(token)\n",
    "        return new_tokens\n",
    "\n",
    "    def lemmatizing(self, tokens: List[str]) -> List[str]:\n",
    "        tagged_tokens = pos_tag(tokens)\n",
    "        lemmatized_tokens = [self.lemmatizer.lemmatize(token, pos) for token, pos in tagged_tokens]\n",
    "        return lemmatized_tokens\n",
    "\n",
    "    def process_hashtags_mentions(self, tokens: List[str]) -> List[str]:\n",
    "        return [token for token in tokens if not token.startswith('#') and not token.startswith('@')]\n",
    "\n",
    "    def replace_country_symbols(self, tokens: List[str]) -> List[str]:\n",
    "        country_symbols = {\n",
    "            'US': 'United States', 'UK': 'United Kingdom', 'IN': 'India', 'CA': 'Canada',\n",
    "            'AU': 'Australia', 'DE': 'Germany', 'FR': 'France', 'ES': 'Spain', 'IT': 'Italy',\n",
    "            'JP': 'Japan', 'CN': 'China', 'BR': 'Brazil', 'RU': 'Russia', 'MX': 'Mexico',\n",
    "            'ZA': 'South Africa', 'KR': 'South Korea', 'AR': 'Argentina', 'SA': 'Saudi Arabia',\n",
    "            'EG': 'Egypt', 'NG': 'Nigeria', 'TR': 'Turkey', 'NL': 'Netherlands', 'SE': 'Sweden',\n",
    "            'CH': 'Switzerland', 'BE': 'Belgium', 'AT': 'Austria', 'DK': 'Denmark', 'FI': 'Finland',\n",
    "            'NO': 'Norway', 'PL': 'Poland', 'IE': 'Ireland', 'NZ': 'New Zealand', 'SG': 'Singapore',\n",
    "            'MY': 'Malaysia', 'TH': 'Thailand', 'PH': 'Philippines', 'ID': 'Indonesia', 'VN': 'Vietnam',\n",
    "            'PK': 'Pakistan', 'BD': 'Bangladesh', 'IR': 'Iran', 'IQ': 'Iraq', 'IL': 'Israel', 'GR': 'Greece',\n",
    "            'PT': 'Portugal', 'CZ': 'Czech Republic', 'HU': 'Hungary', 'RO': 'Romania', 'BG': 'Bulgaria',\n",
    "            'HR': 'Croatia', 'SI': 'Slovenia', 'SK': 'Slovakia', 'UA': 'Ukraine', 'BY': 'Belarus', 'LT': 'Lithuania',\n",
    "            'LV': 'Latvia', 'EE': 'Estonia', 'IS': 'Iceland', 'MT': 'Malta', 'CY': 'Cyprus', 'LK': 'Sri Lanka',\n",
    "            'KE': 'Kenya', 'GH': 'Ghana', 'UG': 'Uganda', 'TZ': 'Tanzania', 'SN': 'Senegal', 'DZ': 'Algeria',\n",
    "            'MA': 'Morocco', 'TN': 'Tunisia', 'AE': 'United Arab Emirates', 'QA': 'Qatar', 'KW': 'Kuwait',\n",
    "            'OM': 'Oman', 'BH': 'Bahrain', 'LB': 'Lebanon', 'JO': 'Jordan', 'SY': 'Syria', 'YE': 'Yemen',\n",
    "            'AF': 'Afghanistan', 'UZ': 'Uzbekistan', 'KZ': 'Kazakhstan', 'KG': 'Kyrgyzstan', 'TJ': 'Tajikistan',\n",
    "            'TM': 'Turkmenistan', 'MN': 'Mongolia', 'KH': 'Cambodia', 'LA': 'Laos', 'MM': 'Myanmar', 'NP': 'Nepal',\n",
    "            'BT': 'Bhutan', 'LK': 'Sri Lanka', 'MV': 'Maldives', 'BN': 'Brunei', 'MO': 'Macau', 'HK': 'Hong Kong',\n",
    "            'TW': 'Taiwan', 'AM': 'Armenia', 'GE': 'Georgia', 'AZ': 'Azerbaijan'\n",
    "        }\n",
    "        return [country_symbols.get(token, token) for token in tokens]\n",
    "\n",
    "    def replace_contractions(self, text: str) -> str:\n",
    "        return contractions.fix(text)\n",
    "\n",
    "\n",
    "\n",
    "    def preprocess(self, text: str) -> str:\n",
    "        \n",
    "        text_tokens = self.tokenize(text)\n",
    "        \n",
    "        operations = [\n",
    "            self.process_hashtags_mentions,       # Step 5: Process hashtags and mentions\n",
    "            self.replace_country_symbols,         # Step 6: Replace country symbols\n",
    "            self.normalize_abbreviations,         # Step 2: Normalize abbreviations\n",
    "            self.remove_markers,                  # Step 9: Remove markers\n",
    "            self.to_lower,                        # Step 1: Convert text to lower case\n",
    "            self.remove_punctuation,              # Step 3: Remove punctuation\n",
    "            self.remove_apostrophe,               # Step 4: Remove apostrophes\n",
    "            self.remove_stop_words,               # Step 8: Remove stop words\n",
    "            self.lemmatizing, \n",
    "        ]\n",
    "\n",
    "        for op in operations:\n",
    "            text_tokens = op(text_tokens)\n",
    "        \n",
    "        new_text = ' '.join(text_tokens)\n",
    "        return new_text\n",
    "\n",
    "class WordEmbeddingEngine:\n",
    "    def __init__(self, vector_size, sg, workers, epochs, text_processor, text_tokenizer):\n",
    "        self.vector_size = vector_size\n",
    "        self.sg = sg\n",
    "        self.workers = workers\n",
    "        self.epochs = epochs\n",
    "        self.text_processor = text_processor\n",
    "        self.text_tokenizer = text_tokenizer\n",
    "        self.word_embedding_model = None\n",
    "        self.documents_vectors = None\n",
    "        self.document_id_mapping = {}\n",
    "\n",
    "    def init_sentences(self, documents):\n",
    "        sentences = []\n",
    "        for doc_id, document in documents.items():\n",
    "            sentences.append(self.text_tokenizer(self.text_processor.preprocess(document)))\n",
    "            self.document_id_mapping[doc_id] = document\n",
    "        return sentences\n",
    "\n",
    "    def train_model(self, documents):\n",
    "        sentences = self.init_sentences(documents)\n",
    "        model = Word2Vec(sentences,\n",
    "                         vector_size=self.vector_size,\n",
    "                         sg=self.sg,\n",
    "                         workers=self.workers,\n",
    "                         epochs=self.epochs)\n",
    "\n",
    "        self.word_embedding_model = model\n",
    "        self.documents_vectors = self.vectorize_documents(sentences)\n",
    "        self.save_model()\n",
    "\n",
    "    def vectorize_documents(self, sentences):\n",
    "        documents_vectors = []\n",
    "        for sentence in sentences:\n",
    "            zero_vector = np.zeros(self.vector_size)\n",
    "            vectors = []\n",
    "            for token in sentence:\n",
    "                if token in self.word_embedding_model.wv:\n",
    "                    try:\n",
    "                        vectors.append(self.word_embedding_model.wv[token])\n",
    "                    except KeyError:\n",
    "                        vectors.append(np.random(self.vector_size))\n",
    "            if vectors:\n",
    "                vectors = np.asarray(vectors)\n",
    "                avg_vec = vectors.mean(axis=0)\n",
    "                documents_vectors.append(avg_vec)\n",
    "            else:\n",
    "                documents_vectors.append(zero_vector)\n",
    "        return documents_vectors\n",
    "\n",
    "    def save_model(self):\n",
    "        with open('word_embedding_model.pickle', 'wb') as f_model:\n",
    "            pickle.dump(self.word_embedding_model, f_model)\n",
    "        with open('document_vectors.pickle', 'wb') as f_vectors:\n",
    "            pickle.dump(self.documents_vectors, f_vectors)\n",
    "        with open('document_id_mapping_vector.pickle', 'wb') as f_mapping:\n",
    "            pickle.dump(self.document_id_mapping, f_mapping)\n",
    "\n",
    "    def load_model(self):\n",
    "        with open('word_embedding_model.pickle', 'rb') as f_model:\n",
    "            self.word_embedding_model = pickle.load(f_model)\n",
    "        with open('document_vectors.pickle', 'rb') as f_vectors:\n",
    "            self.documents_vectors = pickle.load(f_vectors)\n",
    "        with open('document_id_mapping_vector.pickle', 'rb') as f_mapping:\n",
    "            self.document_id_mapping = pickle.load(f_mapping)\n",
    "\n",
    "    def get_query_vector(self, query_text):\n",
    "        preprocessed_query = self.text_processor.preprocess(query_text)\n",
    "        tokens = self.text_tokenizer(preprocessed_query)\n",
    "        vectors = [self.word_embedding_model.wv[token] for token in tokens if token in self.word_embedding_model.wv]\n",
    "        if vectors:\n",
    "            vectors = np.asarray(vectors)\n",
    "            avg_vec = vectors.mean(axis=0)\n",
    "            return avg_vec\n",
    "        else:\n",
    "            return np.zeros(self.vector_size)\n",
    "\n",
    "    def get_results(self, query_text):\n",
    "        query_vector = self.get_query_vector(query_text)\n",
    "        similarities = cosine_similarity([query_vector], self.documents_vectors).flatten()\n",
    "        ranked_indices = np.argsort(-similarities)\n",
    "        result_ids = []\n",
    "        for idx in ranked_indices[:10]:  # Top 10 results\n",
    "            if similarities[idx] >= 0.35:\n",
    "                result_ids.append(list(self.document_id_mapping.keys())[idx])\n",
    "        unordered_results = [{'_id': doc_id, 'text': self.document_id_mapping[doc_id]} for doc_id in result_ids]\n",
    "        return unordered_results\n",
    "    \n",
    "def calculate_MAP(query_id, engine, dataset):\n",
    "    relevant_docs = [qrel.doc_id for qrel in dataset.qrels_iter() if qrel.query_id == query_id]\n",
    "    \n",
    "    ordered_results = []\n",
    "    for query in dataset.queries_iter():\n",
    "        if query.query_id == query_id:\n",
    "            ordered_results = engine.get_results(query.text)\n",
    "            break\n",
    "\n",
    "    pk_sum = 0\n",
    "    total_relevant = 0\n",
    "    for i in range(1, 11):\n",
    "        relevant_ret = 0\n",
    "        for j in range(i):\n",
    "            if j < len(ordered_results) and ordered_results[j]['_id'] in relevant_docs:\n",
    "                relevant_ret += 1\n",
    "        p_at_k = (relevant_ret / i) * (1 if i-1 < len(ordered_results) and ordered_results[i-1]['_id'] in relevant_docs else 0)\n",
    "        pk_sum += p_at_k\n",
    "        if i-1 < len(ordered_results) and ordered_results[i-1]['_id'] in relevant_docs:\n",
    "            total_relevant += 1\n",
    "\n",
    "    return 0 if total_relevant == 0 else pk_sum / total_relevant\n",
    "text_preprocessor = TextPreprocessor()\n",
    "\n",
    "# Load documents\n",
    "dataset = ir_datasets.load('antique/test')\n",
    "documents = {doc.doc_id: doc.text for doc in dataset.docs_iter()}\n",
    "\n",
    "# Initialize WordEmbeddingEngine\n",
    "word_embedding_engine = WordEmbeddingEngine(\n",
    "    vector_size=500, sg=1, workers=4, epochs=35,\n",
    "    text_processor=text_preprocessor,\n",
    "    text_tokenizer=tokenize.word_tokenize\n",
    ")\n",
    "\n",
    "# Train the Word Embedding model (uncomment to train and save the model)\n",
    "word_embedding_engine.train_model(documents)\n",
    "print('Train model done!')\n",
    "\n",
    "# Load the trained Word Embedding model\n",
    "word_embedding_engine.load_model()\n",
    "\n",
    "# Calculate MAP for Word Embedding Engine\n",
    "map_sum_word_embedding = 0\n",
    "queries_ids = {qrel.query_id for qrel in dataset.queries_iter()}\n",
    "for query_id in queries_ids:\n",
    "\n",
    "    map_sum_word_embedding += calculate_MAP(query_id, word_embedding_engine, dataset)\n",
    "\n",
    "mean_average_precision_word_embedding = map_sum_word_embedding / len(queries_ids)\n",
    "print(f\"Mean Average Precision (MAP) for Word Embedding: {mean_average_precision_word_embedding}\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
