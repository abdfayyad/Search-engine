{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/abdfy/anaconda3/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Average Precision (MAP): 0.6567764518770468\n",
      "Mean Precision@10: 0.3434999999999999\n",
      "Mean Recall: 0.1128125178448454\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import string\n",
    "import pickle\n",
    "from typing import List\n",
    "import numpy as np\n",
    "from nltk import tokenize, pos_tag, download\n",
    "from nltk.corpus import stopwords, wordnet\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import ir_datasets\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import pandas as pd\n",
    "from typing import Callable\n",
    "import re\n",
    "import string\n",
    "from typing import Callable, List\n",
    "from nltk.corpus import stopwords, wordnet\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk import pos_tag\n",
    "import numpy as np\n",
    "import inflect\n",
    "import contractions\n",
    "class LemmatizerWithPOSTagger(WordNetLemmatizer):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def _get_wordnet_pos(self, tag: str) -> str:\n",
    "        if tag.startswith('J'):\n",
    "            return wordnet.ADJ\n",
    "        elif tag.startswith('V'):\n",
    "            return wordnet.VERB\n",
    "        elif tag.startswith('N'):\n",
    "            return wordnet.NOUN\n",
    "        elif tag.startswith('R'):\n",
    "            return wordnet.ADV\n",
    "        else:\n",
    "            return wordnet.NOUN\n",
    "\n",
    "    def lemmatize(self, word: str, pos: str = \"n\") -> str:\n",
    "        return super().lemmatize(word, self._get_wordnet_pos(pos))\n",
    "\n",
    "class TextPreprocessor():\n",
    "\n",
    "    def __init__(self, tokenizer: Callable = None) -> None:\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "        if self.tokenizer is None:\n",
    "            self.tokenizer = word_tokenize\n",
    "\n",
    "        self.stopwords_tokens = stopwords.words('english')\n",
    "        self.stemmer = PorterStemmer()\n",
    "        self.lemmatizer = LemmatizerWithPOSTagger()\n",
    "        self.inflect_engine = inflect.engine()\n",
    "\n",
    "    def tokenize(self, text: str) -> List[str]:\n",
    "        tokens = self.tokenizer(text)\n",
    "        return tokens\n",
    "    \n",
    "    def to_lower(self, tokens: List[str]) -> List[str]:\n",
    "        return [token.lower() for token in tokens]\n",
    "\n",
    "    def remove_markers(self, tokens: List[str]) -> List[str]:\n",
    "        return [re.sub(r'\\u00AE', '', token) for token in tokens]\n",
    "\n",
    "    def remove_punctuation(self, tokens: List[str]) -> List[str]:\n",
    "        return [token.translate(str.maketrans('', '', string.punctuation)) for token in tokens]\n",
    "\n",
    "    def replace_under_score_with_space(self, tokens: List[str]) -> List[str]:\n",
    "        return [token.replace('_', ' ') for token in tokens]\n",
    "\n",
    "    def remove_stop_words(self, tokens: List[str]) -> List[str]:\n",
    "        return [token for token in tokens if token not in self.stopwords_tokens and len(token) > 1]\n",
    "\n",
    "    def remove_apostrophe(self, tokens: List[str]) -> List[str]:\n",
    "        return [token.replace(\"'\", \" \") for token in tokens]\n",
    "\n",
    "    def stemming(self, tokens: List[str]) -> List[str]:\n",
    "        return [self.stemmer.stem(token) for token in tokens]\n",
    "\n",
    "    def normalize_abbreviations(self, tokens: List[str]) -> List[str]:\n",
    "        new_tokens = []\n",
    "        resolved_terms = {}\n",
    "        for token in tokens:\n",
    "            if len(token) >= 2:\n",
    "                synsets = wordnet.synsets(token)\n",
    "                if synsets:\n",
    "                    resolved_term = synsets[0].lemmas()[0].name()\n",
    "                    resolved_terms[token] = resolved_term\n",
    "\n",
    "        for token in tokens:\n",
    "            if token in resolved_terms:\n",
    "                new_tokens.append(resolved_terms[token])\n",
    "            else:\n",
    "                new_tokens.append(token)\n",
    "        return new_tokens\n",
    "\n",
    "    def lemmatizing(self, tokens: List[str]) -> List[str]:\n",
    "        tagged_tokens = pos_tag(tokens)\n",
    "        lemmatized_tokens = [self.lemmatizer.lemmatize(token, pos) for token, pos in tagged_tokens]\n",
    "        return lemmatized_tokens\n",
    "\n",
    "    def process_hashtags_mentions(self, tokens: List[str]) -> List[str]:\n",
    "        return [token for token in tokens if not token.startswith('#') and not token.startswith('@')]\n",
    "\n",
    "    def replace_country_symbols(self, tokens: List[str]) -> List[str]:\n",
    "        country_symbols = {\n",
    "            'US': 'United States', 'UK': 'United Kingdom', 'IN': 'India', 'CA': 'Canada',\n",
    "            'AU': 'Australia', 'DE': 'Germany', 'FR': 'France', 'ES': 'Spain', 'IT': 'Italy',\n",
    "            'JP': 'Japan', 'CN': 'China', 'BR': 'Brazil', 'RU': 'Russia', 'MX': 'Mexico',\n",
    "            'ZA': 'South Africa', 'KR': 'South Korea', 'AR': 'Argentina', 'SA': 'Saudi Arabia',\n",
    "            'EG': 'Egypt', 'NG': 'Nigeria', 'TR': 'Turkey', 'NL': 'Netherlands', 'SE': 'Sweden',\n",
    "            'CH': 'Switzerland', 'BE': 'Belgium', 'AT': 'Austria', 'DK': 'Denmark', 'FI': 'Finland',\n",
    "            'NO': 'Norway', 'PL': 'Poland', 'IE': 'Ireland', 'NZ': 'New Zealand', 'SG': 'Singapore',\n",
    "            'MY': 'Malaysia', 'TH': 'Thailand', 'PH': 'Philippines', 'ID': 'Indonesia', 'VN': 'Vietnam',\n",
    "            'PK': 'Pakistan', 'BD': 'Bangladesh', 'IR': 'Iran', 'IQ': 'Iraq', 'IL': 'Israel', 'GR': 'Greece',\n",
    "            'PT': 'Portugal', 'CZ': 'Czech Republic', 'HU': 'Hungary', 'RO': 'Romania', 'BG': 'Bulgaria',\n",
    "            'HR': 'Croatia', 'SI': 'Slovenia', 'SK': 'Slovakia', 'UA': 'Ukraine', 'BY': 'Belarus', 'LT': 'Lithuania',\n",
    "            'LV': 'Latvia', 'EE': 'Estonia', 'IS': 'Iceland', 'MT': 'Malta', 'CY': 'Cyprus', 'LK': 'Sri Lanka',\n",
    "            'KE': 'Kenya', 'GH': 'Ghana', 'UG': 'Uganda', 'TZ': 'Tanzania', 'SN': 'Senegal', 'DZ': 'Algeria',\n",
    "            'MA': 'Morocco', 'TN': 'Tunisia', 'AE': 'United Arab Emirates', 'QA': 'Qatar', 'KW': 'Kuwait',\n",
    "            'OM': 'Oman', 'BH': 'Bahrain', 'LB': 'Lebanon', 'JO': 'Jordan', 'SY': 'Syria', 'YE': 'Yemen',\n",
    "            'AF': 'Afghanistan', 'UZ': 'Uzbekistan', 'KZ': 'Kazakhstan', 'KG': 'Kyrgyzstan', 'TJ': 'Tajikistan',\n",
    "            'TM': 'Turkmenistan', 'MN': 'Mongolia', 'KH': 'Cambodia', 'LA': 'Laos', 'MM': 'Myanmar', 'NP': 'Nepal',\n",
    "            'BT': 'Bhutan', 'LK': 'Sri Lanka', 'MV': 'Maldives', 'BN': 'Brunei', 'MO': 'Macau', 'HK': 'Hong Kong',\n",
    "            'TW': 'Taiwan', 'AM': 'Armenia', 'GE': 'Georgia', 'AZ': 'Azerbaijan'\n",
    "        }\n",
    "        return [country_symbols.get(token, token) for token in tokens]\n",
    "\n",
    "    def replace_contractions(self, text: str) -> str:\n",
    "        return contractions.fix(text)\n",
    "\n",
    "\n",
    "\n",
    "    def preprocess(self, text: str) -> str:\n",
    "        \n",
    "        text_tokens = self.tokenize(text)\n",
    "        \n",
    "        operations = [\n",
    "            self.process_hashtags_mentions,       # Step 5: Process hashtags and mentions\n",
    "            self.replace_country_symbols,         # Step 6: Replace country symbols\n",
    "            self.normalize_abbreviations,         # Step 2: Normalize abbreviations\n",
    "            self.remove_markers,                  # Step 9: Remove markers\n",
    "            self.replace_under_score_with_space ,  # Step 12: Replace underscores with spaces\n",
    "            self.to_lower,                        # Step 1: Convert text to lower case\n",
    "            self.remove_punctuation,              # Step 3: Remove punctuation\n",
    "            self.remove_apostrophe,               # Step 4: Remove apostrophes\n",
    "            self.remove_stop_words,               # Step 8: Remove stop words\n",
    "            self.lemmatizing, \n",
    "        ]\n",
    "\n",
    "        for op in operations:\n",
    "            text_tokens = op(text_tokens)\n",
    "        \n",
    "        new_text = ' '.join(text_tokens)\n",
    "        return new_text\n",
    "\n",
    "class TfidfEngine:\n",
    "    def __init__(self, text_preprocessor):\n",
    "        self.text_preprocessor = text_preprocessor\n",
    "        self.tfidf_matrix = None\n",
    "        self.tfidf_model = None\n",
    "        self.document_id_mapping = {}\n",
    "\n",
    "    def train_model(self, documents):\n",
    "        document_texts = [doc['text'] for doc in documents]\n",
    "        vectorizer = TfidfVectorizer( tokenizer=self.text_preprocessor.tokenizer ,preprocessor=self.text_preprocessor.preprocess)\n",
    "        tfidf_matrix = vectorizer.fit_transform(document_texts)\n",
    "        self.tfidf_matrix = tfidf_matrix\n",
    "        self.tfidf_model = vectorizer\n",
    "        self.save_model(documents)\n",
    "\n",
    "    def save_model(self, documents):\n",
    "        with open('tfidf_model_antique.pickle', 'wb') as f_model:\n",
    "            pickle.dump(self.tfidf_model, f_model)\n",
    "        with open('tfidf_matrix_antique.pickle', 'wb') as f_matrix:\n",
    "            pickle.dump(self.tfidf_matrix, f_matrix)\n",
    "        with open('document_id_mapping_antique.pickle', 'wb') as f_mapping:\n",
    "            pickle.dump({doc['id']: doc['text'] for doc in documents}, f_mapping)\n",
    "\n",
    "    def load_model(self):\n",
    "        with open('tfidf_model_antique.pickle', 'rb') as f_model:\n",
    "            self.tfidf_model = pickle.load(f_model)\n",
    "        with open('tfidf_matrix_antique.pickle', 'rb') as f_matrix:\n",
    "            self.tfidf_matrix = pickle.load(f_matrix)\n",
    "        with open('document_id_mapping_antique.pickle', 'rb') as f_mapping:\n",
    "            self.document_id_mapping = pickle.load(f_mapping)\n",
    "\n",
    "    def query(self, query_text):\n",
    "        preprocessed_query = self.text_preprocessor.preprocess(query_text)\n",
    "        query_vector = self.tfidf_model.transform([preprocessed_query])\n",
    "        return query_vector\n",
    "    \n",
    "    def rank_documents(self, query_vector):\n",
    "        cosine_similarities = cosine_similarity(query_vector, self.tfidf_matrix).flatten()\n",
    "        ranked_indices = np.argsort(-cosine_similarities)\n",
    "        return ranked_indices, cosine_similarities\n",
    "\n",
    "    def get_results(self, query_text):\n",
    "        query_vector = self.query(query_text)\n",
    "        ranked_indices, similarities = self.rank_documents(query_vector)\n",
    "        result_ids = []\n",
    "        for idx in ranked_indices[:10]:  # Top 10 results\n",
    "            if similarities[idx] >= 0.35:\n",
    "                result_ids.append(list(self.document_id_mapping.keys())[idx])\n",
    "        unordered_results = [{'_id': doc_id, 'text': self.document_id_mapping[doc_id]} for doc_id in result_ids]\n",
    "        return unordered_results\n",
    "\n",
    "\n",
    "def calculate_MAP(query_id, tfidf_engine, dataset):\n",
    "    relevant_docs = [qrel.doc_id for qrel in dataset.qrels_iter() if qrel.query_id == query_id]\n",
    "    \n",
    "    ordered_results = []\n",
    "    for query in dataset.queries_iter():\n",
    "        if query.query_id == query_id:\n",
    "            ordered_results = tfidf_engine.get_results(query.text)\n",
    "            break\n",
    "\n",
    "    pk_sum = 0\n",
    "    total_relevant = 0\n",
    "    for i in range(1, 11):\n",
    "        relevant_ret = 0\n",
    "        for j in range(i):\n",
    "            if j < len(ordered_results) and ordered_results[j]['_id'] in relevant_docs:\n",
    "                relevant_ret += 1\n",
    "        p_at_k = (relevant_ret / i) * (1 if i-1 < len(ordered_results) and ordered_results[i-1]['_id'] in relevant_docs else 0)\n",
    "        pk_sum += p_at_k\n",
    "        if i-1 < len(ordered_results) and ordered_results[i-1]['_id'] in relevant_docs:\n",
    "            total_relevant += 1\n",
    "\n",
    "    return 0 if total_relevant == 0 else pk_sum / total_relevant\n",
    "def calculate_precision_at_10(query_id, tfidf_engine, dataset):\n",
    "    relevant_docs = [qrel.doc_id for qrel in dataset.qrels_iter() if qrel.query_id == query_id]\n",
    "    \n",
    "    ordered_results = []\n",
    "    for query in dataset.queries_iter():\n",
    "        if query.query_id == query_id:\n",
    "            ordered_results = tfidf_engine.get_results(query.text)\n",
    "            break\n",
    "\n",
    "    retrieved_docs = [result['_id'] for result in ordered_results[:10]]\n",
    "    relevant_retrieved_docs = [doc for doc in retrieved_docs if doc in relevant_docs]\n",
    "\n",
    "    precision_at_10 = len(relevant_retrieved_docs) / 10\n",
    "    return precision_at_10\n",
    "\n",
    "def calculate_recall(query_id, tfidf_engine, dataset):\n",
    "    relevant_docs = [qrel.doc_id for qrel in dataset.qrels_iter() if qrel.query_id == query_id]\n",
    "    \n",
    "    ordered_results = []\n",
    "    for query in dataset.queries_iter():\n",
    "        if query.query_id == query_id:\n",
    "            ordered_results = tfidf_engine.get_results(query.text)\n",
    "            break\n",
    "\n",
    "    retrieved_docs = [result['_id'] for result in ordered_results]\n",
    "    relevant_retrieved_docs = [doc for doc in retrieved_docs if doc in relevant_docs]\n",
    "\n",
    "    recall = len(relevant_retrieved_docs) / len(relevant_docs) if relevant_docs else 0\n",
    "    return recall\n",
    "\n",
    "# # Initialize TextPreprocessor\n",
    "text_preprocessor = TextPreprocessor()\n",
    "\n",
    "# Load documents\n",
    "documents = []\n",
    "dataset = ir_datasets.load(\"antique/test\")\n",
    "documents = [{'id': doc.doc_id, 'text': doc.text} for doc in dataset.docs_iter()]\n",
    "\n",
    "# # Initialize TfidfEngine with the TextPreprocessor\n",
    "tfidf_engine = TfidfEngine(text_preprocessor)\n",
    "\n",
    "# Train the TF-IDF model (uncomment to train and save the model)\n",
    "tfidf_engine.train_model(documents)\n",
    "\n",
    "# Load the trained model\n",
    "tfidf_engine.load_model()\n",
    "\n",
    "# Calculate MAP, precision@10, and recall for all queries in the dataset\n",
    "dataset = ir_datasets.load(\"antique/test\")\n",
    "\n",
    "queries_ids = {qrel.query_id for qrel in dataset.qrels_iter()}\n",
    "map_sum = 0\n",
    "precision_at_10_sum = 0\n",
    "recall_sum = 0\n",
    "\n",
    "for query_id in queries_ids:\n",
    "    map_sum += calculate_MAP(query_id, tfidf_engine, dataset)\n",
    "    precision_at_10_sum += calculate_precision_at_10(query_id, tfidf_engine, dataset)\n",
    "    recall_sum += calculate_recall(query_id, tfidf_engine, dataset)\n",
    "\n",
    "mean_average_precision = map_sum / len(queries_ids)\n",
    "mean_precision_at_10 = precision_at_10_sum / len(queries_ids)\n",
    "mean_recall = recall_sum / len(queries_ids)\n",
    "\n",
    "print(f\"Mean Average Precision (MAP): {mean_average_precision}\")\n",
    "print(f\"Mean Precision@10: {mean_precision_at_10}\")\n",
    "print(f\"Mean Recall: {mean_recall}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
