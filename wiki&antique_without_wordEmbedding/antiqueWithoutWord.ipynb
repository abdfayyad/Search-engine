{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "import pickle\n",
    "from typing import List\n",
    "import numpy as np\n",
    "from nltk import tokenize, pos_tag, download\n",
    "from nltk.corpus import stopwords, wordnet\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import ir_datasets\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import pandas as pd\n",
    "from typing import Callable\n",
    "import gensim\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.models.callbacks import CallbackAny2Vec\n",
    "import logging\n",
    "\n",
    "\n",
    "class LemmatizerWithPOSTagger(WordNetLemmatizer):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def _get_wordnet_pos(self, tag: str) -> str:\n",
    "        if tag.startswith('J'):\n",
    "            return wordnet.ADJ\n",
    "        elif tag.startswith('V'):\n",
    "            return wordnet.VERB\n",
    "        elif tag.startswith('N'):\n",
    "            return wordnet.NOUN\n",
    "        elif tag.startswith('R'):\n",
    "            return wordnet.ADV\n",
    "        else:\n",
    "            return wordnet.NOUN\n",
    "\n",
    "    def lemmatize(self, word: str, pos: str = \"n\") -> str:\n",
    "        return super().lemmatize(word, self._get_wordnet_pos(pos))\n",
    "\n",
    "class TextPreprocessor():\n",
    "\n",
    "    def __init__(self, tokenizer: Callable = None) -> None:\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "        if self.tokenizer is None:\n",
    "            self.tokenizer = tokenize.word_tokenize\n",
    "\n",
    "        self.stopwords_tokens = stopwords.words('english')\n",
    "        self.stemmer = PorterStemmer()\n",
    "        self.lemmatizer = LemmatizerWithPOSTagger()\n",
    "\n",
    "    def tokenize(self, text: str)-> List[str]:\n",
    "        tokens =self.tokenizer(text)\n",
    "        return tokens\n",
    "    \n",
    "    def to_lower(self, tokens: List[str]) -> List[str]:\n",
    "        lower_tokens = []\n",
    "        for token in tokens:\n",
    "            lower_token = str(np.char.lower(token))\n",
    "            lower_tokens.append(lower_token)\n",
    "        return lower_tokens\n",
    "\n",
    "    \n",
    "    def remove_markers(self, tokens: List[str]) -> List[str]:\n",
    "        new_tokens = []\n",
    "        for token in tokens:\n",
    "            new_tokens.append(re.sub(r'\\u00AE', '', token))\n",
    "        return new_tokens\n",
    "\n",
    "    def remove_punctuation(self, tokens: List[str]) ->  List[str]:\n",
    "        new_tokens = []\n",
    "        for token in tokens:\n",
    "            new_tokens.append(token.translate(str.maketrans('', '', string.punctuation)))\n",
    "        return new_tokens\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def rplace_under_score_with_space(self, tokens: List[str]) -> List[str]:\n",
    "        new_tokens = []\n",
    "        for token in tokens:\n",
    "            new_tokens.append(re.sub(r'_', ' ', token))\n",
    "        return new_tokens\n",
    "\n",
    "    def remove_stop_words(self,tokens: List[str]) -> List[str]:\n",
    "        new_tokens = []\n",
    "        for token in tokens:\n",
    "            if token not in self.stopwords_tokens and len(token) > 1:\n",
    "                new_tokens.append(token)\n",
    "        return new_tokens\n",
    "\n",
    "    def remove_apostrophe(self, tokens: List[str]) -> List[str]:\n",
    "        new_tokens = []\n",
    "        for token in tokens:\n",
    "            new_tokens.append(str(np.char.replace(token, \"'\", \" \")))\n",
    "        return new_tokens\n",
    "\n",
    "    def stemming(self, tokens: List[str]) -> List[str]:\n",
    "        new_tokens = []\n",
    "        for token in tokens:\n",
    "            new_tokens.append(self.stemmer.stem(token))\n",
    "        return new_tokens\n",
    "    \n",
    "    \n",
    "    def normalize_appreviations(self, tokens: List[str]) -> List[str]:\n",
    "        new_tokens = []\n",
    "        resolved_terms = {}\n",
    "        for token in tokens:\n",
    "\n",
    "            if len(token) >= 2:\n",
    "                synsets = wordnet.synsets(token)\n",
    "                if synsets:\n",
    "                    resolved_term = synsets[0].lemmas()[0].name()\n",
    "                    resolved_terms[token] = resolved_term\n",
    "\n",
    "        for abbreviation, resolved_term in resolved_terms.items():\n",
    "            for i in range(len(tokens)):\n",
    "                if tokens[i] == abbreviation:\n",
    "                    tokens[i] = resolved_term\n",
    "                    break\n",
    "\n",
    "        return tokens\n",
    "    \n",
    "    def lemmatizing(self, tokens: List[str]) -> List[str]:\n",
    "        tagged_tokens = pos_tag(tokens)\n",
    "        lemmatized_tokens = [self.lemmatizer.lemmatize(token, pos) for token, pos in tagged_tokens]\n",
    "        return lemmatized_tokens\n",
    "\n",
    "    def replace_country_symbols(self, tokens: List[str]) -> List[str]:\n",
    "            country_symbols = {\n",
    "                'US': 'United States', 'UK': 'United Kingdom', 'IN': 'India', 'CA': 'Canada',\n",
    "                'AU': 'Australia', 'DE': 'Germany', 'FR': 'France', 'ES': 'Spain', 'IT': 'Italy',\n",
    "                'JP': 'Japan', 'CN': 'China', 'BR': 'Brazil', 'RU': 'Russia', 'MX': 'Mexico',\n",
    "                'ZA': 'South Africa', 'KR': 'South Korea', 'AR': 'Argentina', 'SA': 'Saudi Arabia',\n",
    "                'EG': 'Egypt', 'NG': 'Nigeria', 'TR': 'Turkey', 'NL': 'Netherlands', 'SE': 'Sweden',\n",
    "                'CH': 'Switzerland', 'BE': 'Belgium', 'AT': 'Austria', 'DK': 'Denmark', 'FI': 'Finland',\n",
    "                'NO': 'Norway', 'PL': 'Poland', 'IE': 'Ireland', 'NZ': 'New Zealand', 'SG': 'Singapore',\n",
    "                'MY': 'Malaysia', 'TH': 'Thailand', 'PH': 'Philippines', 'ID': 'Indonesia', 'VN': 'Vietnam',\n",
    "                'PK': 'Pakistan', 'BD': 'Bangladesh', 'IR': 'Iran', 'IQ': 'Iraq', 'IL': 'Israel', 'GR': 'Greece',\n",
    "                'PT': 'Portugal', 'CZ': 'Czech Republic', 'HU': 'Hungary', 'RO': 'Romania', 'BG': 'Bulgaria',\n",
    "                'HR': 'Croatia', 'SI': 'Slovenia', 'SK': 'Slovakia', 'UA': 'Ukraine', 'BY': 'Belarus', 'LT': 'Lithuania',\n",
    "                'LV': 'Latvia', 'EE': 'Estonia', 'IS': 'Iceland', 'MT': 'Malta', 'CY': 'Cyprus', 'LK': 'Sri Lanka',\n",
    "                'KE': 'Kenya', 'GH': 'Ghana', 'UG': 'Uganda', 'TZ': 'Tanzania', 'SN': 'Senegal', 'DZ': 'Algeria',\n",
    "                'MA': 'Morocco', 'TN': 'Tunisia', 'AE': 'United Arab Emirates', 'QA': 'Qatar', 'KW': 'Kuwait',\n",
    "                'OM': 'Oman', 'BH': 'Bahrain', 'LB': 'Lebanon', 'JO': 'Jordan', 'SY': 'Syria', 'YE': 'Yemen',\n",
    "                'AF': 'Afghanistan', 'UZ': 'Uzbekistan', 'KZ': 'Kazakhstan', 'KG': 'Kyrgyzstan', 'TJ': 'Tajikistan',\n",
    "                'TM': 'Turkmenistan', 'MN': 'Mongolia', 'KH': 'Cambodia', 'LA': 'Laos', 'MM': 'Myanmar', 'NP': 'Nepal',\n",
    "                'BT': 'Bhutan', 'LK': 'Sri Lanka', 'MV': 'Maldives', 'BN': 'Brunei', 'MO': 'Macau', 'HK': 'Hong Kong',\n",
    "                'TW': 'Taiwan', 'AM': 'Armenia', 'GE': 'Georgia', 'AZ': 'Azerbaijan'\n",
    "            }\n",
    "            return [country_symbols.get(token, token) for token in tokens]\n",
    "\n",
    "    def process_hashtags_mentions(self, tokens: List[str]) -> List[str]:\n",
    "            new_tokens = [token for token in tokens if not token.startswith('#') and not token.startswith('@')]\n",
    "            return new_tokens\n",
    "\n",
    "    def normalize_abbreviations(self, tokens: List[str]) -> List[str]:\n",
    "            new_tokens = []\n",
    "            resolved_terms = {}\n",
    "            for token in tokens:\n",
    "                if len(token) >= 2:\n",
    "                    synsets = wordnet.synsets(token)\n",
    "                    if synsets:\n",
    "                        resolved_term = synsets[0].lemmas()[0].name()\n",
    "                        resolved_terms[token] = resolved_term\n",
    "\n",
    "            for abbreviation, resolved_term in resolved_terms.items():\n",
    "                tokens = [resolved_term if token == abbreviation else token for token in tokens]\n",
    "            return tokens\n",
    "    def preprocess(self, text: str) -> str:\n",
    "        operations = [\n",
    "            self.process_hashtags_mentions,       # Step 5: Process hashtags and mentions\n",
    "            self.replace_country_symbols,         # Step 6: Replace country symbols\n",
    "            self.normalize_abbreviations,         # Step 2: Normalize abbreviations\n",
    "            self.remove_markers,                  # Step 9: Remove markers\n",
    "            self.to_lower,                        # Step 1: Convert text to lower case\n",
    "            self.remove_punctuation,              # Step 3: Remove punctuation\n",
    "            self.remove_apostrophe,               # Step 4: Remove apostrophes\n",
    "            self.remove_stop_words,               # Step 8: Remove stop words\n",
    "            self.lemmatizing, \n",
    "        ]\n",
    "        text_tokens=self.tokenize(text)\n",
    "        for op in operations:\n",
    "              text_tokens=op(text_tokens)\n",
    "    \n",
    "        new_text=\"\"\n",
    "        new_text = ' '.join(text_tokens)\n",
    "            \n",
    "        return new_text\n",
    "\n",
    "class TfidfEngine:\n",
    "    def __init__(self, text_preprocessor):\n",
    "        self.text_preprocessor = text_preprocessor\n",
    "        self.tfidf_matrix = None\n",
    "        self.tfidf_model = None\n",
    "        self.document_id_mapping = {}\n",
    "\n",
    "    def train_model(self, documents):\n",
    "        document_texts = [doc['text'] for doc in documents]\n",
    "        vectorizer = TfidfVectorizer(preprocessor=self.text_preprocessor.preprocess, tokenizer=self.text_preprocessor.tokenizer)\n",
    "        tfidf_matrix = vectorizer.fit_transform(document_texts)\n",
    "        self.tfidf_matrix = tfidf_matrix\n",
    "        self.tfidf_model = vectorizer\n",
    "        self.save_model(documents)\n",
    "\n",
    "    def save_model(self, documents):\n",
    "        with open('tfidf_model2.pickle', 'wb') as f_model:\n",
    "            pickle.dump(self.tfidf_model, f_model)\n",
    "        with open('tfidf_matrix2.pickle', 'wb') as f_matrix:\n",
    "            pickle.dump(self.tfidf_matrix, f_matrix)\n",
    "        with open('document_id_mapping2.pickle', 'wb') as f_mapping:\n",
    "            pickle.dump({doc['id']: doc['text'] for doc in documents}, f_mapping)\n",
    "\n",
    "    def load_model(self):\n",
    "        with open('tfidf_model2.pickle', 'rb') as f_model:\n",
    "            self.tfidf_model = pickle.load(f_model)\n",
    "        with open('tfidf_matrix2.pickle', 'rb') as f_matrix:\n",
    "            self.tfidf_matrix = pickle.load(f_matrix)\n",
    "        with open('document_id_mapping2.pickle', 'rb') as f_mapping:\n",
    "            self.document_id_mapping = pickle.load(f_mapping)\n",
    "\n",
    "    def query(self, query_text):\n",
    "        preprocessed_query = self.text_preprocessor.preprocess(query_text)\n",
    "        query_vector = self.tfidf_model.transform([preprocessed_query])\n",
    "        return query_vector\n",
    "    \n",
    "    def rank_documents(self, query_vector):\n",
    "        cosine_similarities = cosine_similarity(query_vector, self.tfidf_matrix).flatten()\n",
    "        ranked_indices = np.argsort(-cosine_similarities)\n",
    "        return ranked_indices, cosine_similarities\n",
    "\n",
    "    def get_results(self, query_text):\n",
    "        query_vector = self.query(query_text)\n",
    "        ranked_indices, similarities = self.rank_documents(query_vector)\n",
    "        result_ids = []\n",
    "        for idx in ranked_indices[:10]:  # Top 10 results\n",
    "            if similarities[idx] >= 0.35:\n",
    "                result_ids.append(list(self.document_id_mapping.keys())[idx])\n",
    "        unordered_results = [{'_id': doc_id, 'text': self.document_id_mapping[doc_id]} for doc_id in result_ids]\n",
    "        return unordered_results\n",
    "\n",
    "\n",
    "def calculate_MAP(query_id, tfidf_engine, dataset):\n",
    "    relevant_docs = [qrel.doc_id for qrel in dataset.qrels_iter() if qrel.query_id == query_id]\n",
    "    \n",
    "    ordered_results = []\n",
    "    for query in dataset.queries_iter():\n",
    "        if query.query_id == query_id:\n",
    "            ordered_results = tfidf_engine.get_results(query.text)\n",
    "            break\n",
    "\n",
    "    pk_sum = 0\n",
    "    total_relevant = 0\n",
    "    for i in range(1, 11):\n",
    "        relevant_ret = 0\n",
    "        for j in range(i):\n",
    "            if j < len(ordered_results) and ordered_results[j]['_id'] in relevant_docs:\n",
    "                relevant_ret += 1\n",
    "        p_at_k = (relevant_ret / i) * (1 if i-1 < len(ordered_results) and ordered_results[i-1]['_id'] in relevant_docs else 0)\n",
    "        pk_sum += p_at_k\n",
    "        if i-1 < len(ordered_results) and ordered_results[i-1]['_id'] in relevant_docs:\n",
    "            total_relevant += 1\n",
    "\n",
    "    return 0 if total_relevant == 0 else pk_sum / total_relevant\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/abdfy/anaconda3/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Average Precision (MAP): 0.2483684018476248\n",
      "Mean Precision@10: 0.08326463314097184\n",
      "Mean Recall: 0.09534191839072663\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "def calculate_MAP(query_id, tfidf_engine, dataset):\n",
    "    relevant_docs = [qrel.doc_id for qrel in dataset.qrels_iter() if qrel.query_id == query_id]\n",
    "    \n",
    "    ordered_results = []\n",
    "    for query in dataset.queries_iter():\n",
    "        if query.query_id == query_id:\n",
    "            ordered_results = tfidf_engine.get_results(query.text)\n",
    "            break\n",
    "\n",
    "    pk_sum = 0\n",
    "    total_relevant = 0\n",
    "    for i in range(1, 11):\n",
    "        relevant_ret = 0\n",
    "        for j in range(i):\n",
    "            if j < len(ordered_results) and ordered_results[j]['_id'] in relevant_docs:\n",
    "                relevant_ret += 1\n",
    "        p_at_k = (relevant_ret / i) * (1 if i-1 < len(ordered_results) and ordered_results[i-1]['_id'] in relevant_docs else 0)\n",
    "        pk_sum += p_at_k\n",
    "        if i-1 < len(ordered_results) and ordered_results[i-1]['_id'] in relevant_docs:\n",
    "            total_relevant += 1\n",
    "\n",
    "    return 0 if total_relevant == 0 else pk_sum / total_relevant\n",
    "def calculate_precision_at_10(query_id, tfidf_engine, dataset):\n",
    "    relevant_docs = [qrel.doc_id for qrel in dataset.qrels_iter() if qrel.query_id == query_id]\n",
    "    \n",
    "    ordered_results = []\n",
    "    for query in dataset.queries_iter():\n",
    "        if query.query_id == query_id:\n",
    "            ordered_results = tfidf_engine.get_results(query.text)\n",
    "            break\n",
    "\n",
    "    retrieved_docs = [result['_id'] for result in ordered_results[:10]]\n",
    "    relevant_retrieved_docs = [doc for doc in retrieved_docs if doc in relevant_docs]\n",
    "\n",
    "    precision_at_10 = len(relevant_retrieved_docs) / 10\n",
    "    return precision_at_10\n",
    "\n",
    "def calculate_recall(query_id, tfidf_engine, dataset):\n",
    "    relevant_docs = [qrel.doc_id for qrel in dataset.qrels_iter() if qrel.query_id == query_id]\n",
    "    \n",
    "    ordered_results = []\n",
    "    for query in dataset.queries_iter():\n",
    "        if query.query_id == query_id:\n",
    "            ordered_results = tfidf_engine.get_results(query.text)\n",
    "            break\n",
    "\n",
    "    retrieved_docs = [result['_id'] for result in ordered_results]\n",
    "    relevant_retrieved_docs = [doc for doc in retrieved_docs if doc in relevant_docs]\n",
    "\n",
    "    recall = len(relevant_retrieved_docs) / len(relevant_docs) if relevant_docs else 0\n",
    "    return recall\n",
    "\n",
    "# # Initialize TextPreprocessor\n",
    "text_preprocessor = TextPreprocessor()\n",
    "\n",
    "# Load documents\n",
    "dataset = ir_datasets.load(\"antique/train\")\n",
    "documents = [{'id': doc.doc_id, 'text': doc.text} for doc in dataset.docs_iter()]\n",
    "\n",
    "tfidf_engine = TfidfEngine(text_preprocessor)\n",
    "\n",
    "tfidf_engine.train_model(documents)\n",
    "\n",
    "tfidf_engine.load_model()\n",
    "\n",
    "dataset = ir_datasets.load(\"antique/train\")\n",
    "\n",
    "queries_ids = {qrel.query_id for qrel in dataset.qrels_iter()}\n",
    "map_sum = 0\n",
    "precision_at_10_sum = 0\n",
    "recall_sum = 0\n",
    "\n",
    "for query_id in queries_ids:\n",
    "    map_sum += calculate_MAP(query_id, tfidf_engine, dataset)\n",
    "    precision_at_10_sum += calculate_precision_at_10(query_id, tfidf_engine, dataset)\n",
    "    recall_sum += calculate_recall(query_id, tfidf_engine, dataset)\n",
    "\n",
    "mean_average_precision = map_sum / len(queries_ids)\n",
    "mean_precision_at_10 = precision_at_10_sum / len(queries_ids)\n",
    "mean_recall = recall_sum / len(queries_ids)\n",
    "\n",
    "print(f\"Mean Average Precision (MAP): {mean_average_precision}\")\n",
    "print(f\"Mean Precision@10: {mean_precision_at_10}\")\n",
    "print(f\"Mean Recall: {mean_recall}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
