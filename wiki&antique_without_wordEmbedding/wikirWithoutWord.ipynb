{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document ID 851454_22: smoke what cigarettes or reefer?? cigarettes i have never and will never smoke and as for the other well it relaxes you\n",
      "Similarity: 0.5663244839508543\n",
      "Document ID 1603419_6: Got a cigarette?\n",
      "Similarity: 0.5629552401929355\n",
      "Document ID 20085_8: Cigarettes and alcohol. One for each hand and not illegal.\n",
      "Similarity: 0.5105578255383663\n",
      "Document ID 2273650_4: when you light up that cigarette.\n",
      "Similarity: 0.4987626942223903\n",
      "Document ID 1910631_1: Always wanting a cigarette\n",
      "Similarity: 0.4712118920399412\n",
      "Document ID 3638873_10: wash it down with a joint\n",
      "Similarity: 0.46114641500013825\n",
      "Document ID 3124812_0: ball joint\n",
      "Similarity: 0.4592886693133129\n",
      "Document ID 2907121_2: A chicken wing has 3 joints split the 2 largest joints apart and throw away the smallest tip joint.\n",
      "Similarity: 0.45595486493856585\n",
      "Document ID 717237_2: What you actually pop is your joint. When an air bubble forms in your joint and it pops. You feel a need to pop your joint because an air bubble in your joint will obviously make that joint feel pretty off kilter.\n",
      "Similarity: 0.44851407613257005\n",
      "Document ID 1288793_4: STOP BUYING CIGARETTES!\n",
      "Similarity: 0.44717896091472764\n",
      "Document ID 3929946_2: its a substance in cigarettes\n",
      "Similarity: 0.445358599944629\n",
      "Document ID 2143173_2: There isn't a difference between a joint and a spliff or a blunt and a spliff. A spliff is just pott rolled in some type of rolling paper. A joint is pott rolled in cigarette paper and a blunt is pott rolled in cigar leaves.\n",
      "Similarity: 0.43017748155144275\n",
      "Document ID 4267350_5: You need ball joints.\n",
      "Similarity: 0.4280960063780243\n",
      "Document ID 3385681_2: its addicting just like cigarettes\n",
      "Similarity: 0.42607351529993504\n",
      "Document ID 634833_10: Can I bum a cigarette?\n",
      "Similarity: 0.4191548314907431\n",
      "Document ID 574551_2: its easy.. don't buy cigarette..\n",
      "Similarity: 0.41550459624780317\n",
      "Document ID 1416663_1: by hand\n",
      "Similarity: 0.4124918337776385\n",
      "Document ID 173856_0: By Hand.\n",
      "Similarity: 0.4124918337776385\n",
      "Document ID 2373411_18: With both hands.\n",
      "Similarity: 0.4124918337776385\n",
      "Document ID 521451_3: with your hands!!!\n",
      "Similarity: 0.4124918337776385\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import string\n",
    "import pickle\n",
    "from typing import List\n",
    "import numpy as np\n",
    "from nltk import tokenize, pos_tag\n",
    "from nltk.corpus import stopwords, wordnet\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import ir_datasets\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "class TextPreprocessor:\n",
    "    def __init__(self):\n",
    "        self.tokenizer = tokenize.word_tokenize\n",
    "        self.stopwords_tokens = set(stopwords.words('english'))\n",
    "        self.stemmer = PorterStemmer()\n",
    "        self.lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "    def preprocess(self, text: str) -> str:\n",
    "        text = self._to_lower(text)\n",
    "        text = self._remove_punctuation(text)\n",
    "        text = self._remove_apostrophe(text)\n",
    "        text = self._remove_stop_words(text)\n",
    "        text = self._remove_markers(text)\n",
    "        text = self._stemming(text)\n",
    "        text = self._lemmatizing(text)\n",
    "        text = self._normalize_abbreviations(text)\n",
    "        return text\n",
    "\n",
    "    def _to_lower(self, text: str) -> str:\n",
    "        return str(np.char.lower(text))\n",
    "\n",
    "    def _remove_punctuation(self, text: str) -> str:\n",
    "        return text.translate(str.maketrans('', '', string.punctuation))\n",
    "\n",
    "    def _remove_apostrophe(self, text: str) -> str:\n",
    "        return str(np.char.replace(text, \"'\", \" \"))\n",
    "\n",
    "    def _remove_stop_words(self, text: str) -> str:\n",
    "        return ' '.join([token for token in text.split() if token not in self.stopwords_tokens and len(token) > 1])\n",
    "\n",
    "    def _remove_markers(self, text: str) -> str:\n",
    "        return re.sub(r'\\u00AE', '', text)\n",
    "\n",
    "    def _stemming(self, text: str) -> str:\n",
    "        return ' '.join([self.stemmer.stem(token) for token in text.split()])\n",
    "\n",
    "    def _get_wordnet_pos(self, tag: str) -> str:\n",
    "        if tag.startswith('J'):\n",
    "            return wordnet.ADJ\n",
    "        elif tag.startswith('V'):\n",
    "            return wordnet.VERB\n",
    "        elif tag.startswith('N'):\n",
    "            return wordnet.NOUN\n",
    "        elif tag.startswith('R'):\n",
    "            return wordnet.ADV\n",
    "        else:\n",
    "            return wordnet.NOUN\n",
    "\n",
    "    def _lemmatizing(self, text: str) -> str:\n",
    "        tagged_tokens = pos_tag(text.split())\n",
    "        lemmatized_tokens = [self.lemmatizer.lemmatize(token, self._get_wordnet_pos(pos)) for token, pos in tagged_tokens]\n",
    "        return ' '.join(lemmatized_tokens)\n",
    "\n",
    "    def _normalize_abbreviations(self, text: str) -> str:\n",
    "        resolved_terms = {}\n",
    "        for token in text.split():\n",
    "            if len(token) >= 2:\n",
    "                synsets = wordnet.synsets(token)\n",
    "                if synsets:\n",
    "                    resolved_term = synsets[0].lemmas()[0].name()\n",
    "                    resolved_terms[token] = resolved_term\n",
    "\n",
    "        for token in text.split():\n",
    "            if token in resolved_terms:\n",
    "                text = text.replace(token, resolved_terms[token])\n",
    "\n",
    "        return text\n",
    "\n",
    "\n",
    "class TfidfEngine:\n",
    "    def __init__(self, text_preprocessor):\n",
    "        self.text_preprocessor = text_preprocessor\n",
    "        self.tfidf_matrix = None\n",
    "        self.tfidf_model = None\n",
    "        self.document_id_mapping = {}\n",
    "\n",
    "    def train_model(self, documents):\n",
    "        document_texts = [doc['text'] for doc in documents]\n",
    "        vectorizer = TfidfVectorizer(preprocessor=self.text_preprocessor.preprocess, tokenizer=self.text_preprocessor.tokenizer)\n",
    "        tfidf_matrix = vectorizer.fit_transform(document_texts)\n",
    "        self.tfidf_matrix = tfidf_matrix\n",
    "        self.tfidf_model = vectorizer\n",
    "        self.save_model(documents)\n",
    "\n",
    "    def save_model(self, documents):\n",
    "        with open('tfidf_model.pickle', 'wb') as f_model:\n",
    "            pickle.dump(self.tfidf_model, f_model)\n",
    "        with open('tfidf_matrix.pickle', 'wb') as f_matrix:\n",
    "            pickle.dump(self.tfidf_matrix, f_matrix)\n",
    "        with open('document_id_mapping.pickle', 'wb') as f_mapping:\n",
    "            pickle.dump({doc['id']: doc['text'] for doc in documents}, f_mapping)\n",
    "\n",
    "    def load_model(self):\n",
    "        with open('tfidf_model.pickle', 'rb') as f_model:\n",
    "            self.tfidf_model = pickle.load(f_model)\n",
    "        with open('tfidf_matrix.pickle', 'rb') as f_matrix:\n",
    "            self.tfidf_matrix = pickle.load(f_matrix)\n",
    "        with open('document_id_mapping.pickle', 'rb') as f_mapping:\n",
    "            self.document_id_mapping = pickle.load(f_mapping)\n",
    "\n",
    "    def query(self, query_text):\n",
    "        preprocessed_query = self.text_preprocessor.preprocess(query_text)\n",
    "        query_vector = self.tfidf_model.transform([preprocessed_query])\n",
    "        return query_vector\n",
    "    \n",
    "    def rank_documents(self, query_vector):\n",
    "        cosine_similarities = cosine_similarity(query_vector, self.tfidf_matrix).flatten()\n",
    "        ranked_indices = np.argsort(-cosine_similarities)\n",
    "        return ranked_indices, cosine_similarities\n",
    "\n",
    "# Usage example:\n",
    "# Initialize TextPreprocessor\n",
    "text_preprocessor = TextPreprocessor()\n",
    "\n",
    "# Load documents\n",
    "dataset = ir_datasets.load('antique/train')\n",
    "documents = [{'id': doc.doc_id, 'text': doc.text} for doc in dataset.docs_iter()]\n",
    "\n",
    "# Initialize TfidfEngine with the TextPreprocessor\n",
    "tfidf_engine = TfidfEngine(text_preprocessor)\n",
    "\n",
    "# Train the TF-IDF model\n",
    "# tfidf_engine.train_model(documents)\n",
    "\n",
    "# Save the trained model (this step is already done inside train_model)\n",
    "# tfidf_engine.save_model(documents)\n",
    "\n",
    "# Load the trained model\n",
    "tfidf_engine.load_model()\n",
    "\n",
    "# Query processing and ranking\n",
    "query_text = \"what is the difference between a cigarette and a hand rolled joint?\"\n",
    "query_vector = tfidf_engine.query(query_text)\n",
    "ranked_indices, similarities = tfidf_engine.rank_documents(query_vector)\n",
    "\n",
    "# Output the ranked documents\n",
    "for idx in ranked_indices[:20]:  # Show top 20 results\n",
    "    doc_id = list(tfidf_engine.document_id_mapping.keys())[idx]\n",
    "    doc_text = tfidf_engine.document_id_mapping[doc_id]\n",
    "    print(f\"Document ID {doc_id}: {doc_text}\")\n",
    "    print(f\"Similarity: {similarities[idx]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "import pickle\n",
    "from typing import List\n",
    "import numpy as np\n",
    "from nltk import tokenize, pos_tag, download\n",
    "from nltk.corpus import stopwords, wordnet\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import ir_datasets\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import pandas as pd\n",
    "from typing import Callable\n",
    "class LemmatizerWithPOSTagger(WordNetLemmatizer):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def _get_wordnet_pos(self, tag: str) -> str:\n",
    "        if tag.startswith('J'):\n",
    "            return wordnet.ADJ\n",
    "        elif tag.startswith('V'):\n",
    "            return wordnet.VERB\n",
    "        elif tag.startswith('N'):\n",
    "            return wordnet.NOUN\n",
    "        elif tag.startswith('R'):\n",
    "            return wordnet.ADV\n",
    "        else:\n",
    "            return wordnet.NOUN\n",
    "\n",
    "    def lemmatize(self, word: str, pos: str = \"n\") -> str:\n",
    "        return super().lemmatize(word, self._get_wordnet_pos(pos))\n",
    "\n",
    "class TextPreprocessor():\n",
    "\n",
    "    def __init__(self, tokenizer: Callable = None) -> None:\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "        if self.tokenizer is None:\n",
    "            self.tokenizer = tokenize.word_tokenize\n",
    "\n",
    "        self.stopwords_tokens = stopwords.words('english')\n",
    "        self.stemmer = PorterStemmer()\n",
    "        self.lemmatizer = LemmatizerWithPOSTagger()\n",
    "\n",
    "    def tokenize(self, text: str)-> List[str]:\n",
    "        tokens =self.tokenizer(text)\n",
    "        return tokens\n",
    "    \n",
    "    def to_lower(self, tokens: List[str]) -> List[str]:\n",
    "        lower_tokens = []\n",
    "        for token in tokens:\n",
    "            lower_token = str(np.char.lower(token))\n",
    "            lower_tokens.append(lower_token)\n",
    "        return lower_tokens\n",
    "\n",
    "    \n",
    "    def remove_markers(self, tokens: List[str]) -> List[str]:\n",
    "        new_tokens = []\n",
    "        for token in tokens:\n",
    "            new_tokens.append(re.sub(r'\\u00AE', '', token))\n",
    "        return new_tokens\n",
    "\n",
    "    def remove_punctuation(self, tokens: List[str]) ->  List[str]:\n",
    "        new_tokens = []\n",
    "        for token in tokens:\n",
    "            new_tokens.append(token.translate(str.maketrans('', '', string.punctuation)))\n",
    "        return new_tokens\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def rplace_under_score_with_space(self, tokens: List[str]) -> List[str]:\n",
    "        new_tokens = []\n",
    "        for token in tokens:\n",
    "            new_tokens.append(re.sub(r'_', ' ', token))\n",
    "        return new_tokens\n",
    "\n",
    "    def remove_stop_words(self,tokens: List[str]) -> List[str]:\n",
    "        new_tokens = []\n",
    "        for token in tokens:\n",
    "            if token not in self.stopwords_tokens and len(token) > 1:\n",
    "                new_tokens.append(token)\n",
    "        return new_tokens\n",
    "\n",
    "    def remove_apostrophe(self, tokens: List[str]) -> List[str]:\n",
    "        new_tokens = []\n",
    "        for token in tokens:\n",
    "            new_tokens.append(str(np.char.replace(token, \"'\", \" \")))\n",
    "        return new_tokens\n",
    "\n",
    "    def stemming(self, tokens: List[str]) -> List[str]:\n",
    "        new_tokens = []\n",
    "        for token in tokens:\n",
    "            new_tokens.append(self.stemmer.stem(token))\n",
    "        return new_tokens\n",
    "    \n",
    "    \n",
    "    def normalize_appreviations(self, tokens: List[str]) -> List[str]:\n",
    "        new_tokens = []\n",
    "        resolved_terms = {}\n",
    "        for token in tokens:\n",
    "\n",
    "            if len(token) >= 2:\n",
    "                synsets = wordnet.synsets(token)\n",
    "                if synsets:\n",
    "                    resolved_term = synsets[0].lemmas()[0].name()\n",
    "                    resolved_terms[token] = resolved_term\n",
    "\n",
    "        for abbreviation, resolved_term in resolved_terms.items():\n",
    "            for i in range(len(tokens)):\n",
    "                if tokens[i] == abbreviation:\n",
    "                    tokens[i] = resolved_term\n",
    "                    break\n",
    "\n",
    "        return tokens\n",
    "    \n",
    "    def lemmatizing(self, tokens: List[str]) -> List[str]:\n",
    "        tagged_tokens = pos_tag(tokens)\n",
    "        lemmatized_tokens = [self.lemmatizer.lemmatize(token, pos) for token, pos in tagged_tokens]\n",
    "        return lemmatized_tokens\n",
    "\n",
    "\n",
    "    def preprocess(self, text: str) -> str:\n",
    "        operations = [\n",
    "            self.to_lower,\n",
    "            self.remove_punctuation,\n",
    "            self.remove_apostrophe,\n",
    "            self.remove_stop_words,\n",
    "            self.remove_markers,\n",
    "            self.stemming,\n",
    "            self.lemmatizing,\n",
    "            self.normalize_appreviations, \n",
    "            self.to_lower,\n",
    "            self.rplace_under_score_with_space\n",
    "        ]\n",
    "        text_tokens=self.tokenize(text)\n",
    "        for op in operations:\n",
    "              text_tokens=op(text_tokens)\n",
    "    \n",
    "        new_text=\"\"\n",
    "        new_text = ' '.join(text_tokens)\n",
    "            \n",
    "        return new_text\n",
    "\n",
    "class TfidfEngine:\n",
    "    def __init__(self, text_preprocessor):\n",
    "        self.text_preprocessor = text_preprocessor\n",
    "        self.tfidf_matrix = None\n",
    "        self.tfidf_model = None\n",
    "        self.document_id_mapping = {}\n",
    "\n",
    "    def train_model(self, documents):\n",
    "        document_texts = [doc['text'] for doc in documents]\n",
    "        vectorizer = TfidfVectorizer(preprocessor=self.text_preprocessor.preprocess, tokenizer=self.text_preprocessor.tokenizer)\n",
    "        tfidf_matrix = vectorizer.fit_transform(document_texts)\n",
    "        self.tfidf_matrix = tfidf_matrix\n",
    "        self.tfidf_model = vectorizer\n",
    "        self.save_model(documents)\n",
    "\n",
    "    def save_model(self, documents):\n",
    "        with open('tfidf_model.pickle', 'wb') as f_model:\n",
    "            pickle.dump(self.tfidf_model, f_model)\n",
    "        with open('tfidf_matrix.pickle', 'wb') as f_matrix:\n",
    "            pickle.dump(self.tfidf_matrix, f_matrix)\n",
    "        with open('document_id_mapping.pickle', 'wb') as f_mapping:\n",
    "            pickle.dump({doc['id']: doc['text'] for doc in documents}, f_mapping)\n",
    "\n",
    "    def load_model(self):\n",
    "        with open('tfidf_model.pickle', 'rb') as f_model:\n",
    "            self.tfidf_model = pickle.load(f_model)\n",
    "        with open('tfidf_matrix.pickle', 'rb') as f_matrix:\n",
    "            self.tfidf_matrix = pickle.load(f_matrix)\n",
    "        with open('document_id_mapping.pickle', 'rb') as f_mapping:\n",
    "            self.document_id_mapping = pickle.load(f_mapping)\n",
    "\n",
    "    def query(self, query_text):\n",
    "        preprocessed_query = self.text_preprocessor.preprocess(query_text)\n",
    "        query_vector = self.tfidf_model.transform([preprocessed_query])\n",
    "        return query_vector\n",
    "    \n",
    "    def rank_documents(self, query_vector):\n",
    "        cosine_similarities = cosine_similarity(query_vector, self.tfidf_matrix).flatten()\n",
    "        ranked_indices = np.argsort(-cosine_similarities)\n",
    "        return ranked_indices, cosine_similarities\n",
    "\n",
    "    def get_results(self, query_text):\n",
    "        query_vector = self.query(query_text)\n",
    "        ranked_indices, similarities = self.rank_documents(query_vector)\n",
    "        result_ids = []\n",
    "        for idx in ranked_indices[:10]:  # Top 10 results\n",
    "            if similarities[idx] >= 0.35:\n",
    "                result_ids.append(list(self.document_id_mapping.keys())[idx])\n",
    "        unordered_results = [{'_id': doc_id, 'text': self.document_id_mapping[doc_id]} for doc_id in result_ids]\n",
    "        return unordered_results\n",
    "\n",
    "\n",
    "def calculate_MAP(query_id, tfidf_engine, dataset):\n",
    "    relevant_docs = [qrel.doc_id for qrel in dataset.qrels_iter() if qrel.query_id == query_id]\n",
    "    \n",
    "    ordered_results = []\n",
    "    for query in dataset.queries_iter():\n",
    "        if query.query_id == query_id:\n",
    "            ordered_results = tfidf_engine.get_results(query.text)\n",
    "            break\n",
    "\n",
    "    pk_sum = 0\n",
    "    total_relevant = 0\n",
    "    for i in range(1, 11):\n",
    "        relevant_ret = 0\n",
    "        for j in range(i):\n",
    "            if j < len(ordered_results) and ordered_results[j]['_id'] in relevant_docs:\n",
    "                relevant_ret += 1\n",
    "        p_at_k = (relevant_ret / i) * (1 if i-1 < len(ordered_results) and ordered_results[i-1]['_id'] in relevant_docs else 0)\n",
    "        pk_sum += p_at_k\n",
    "        if i-1 < len(ordered_results) and ordered_results[i-1]['_id'] in relevant_docs:\n",
    "            total_relevant += 1\n",
    "\n",
    "    return 0 if total_relevant == 0 else pk_sum / total_relevant\n",
    "\n",
    "# Initialize TextPreprocessor\n",
    "# text_preprocessor = TextPreprocessor()\n",
    "\n",
    "# # Load documents\n",
    "# dataset = ir_datasets.load('wikir/en1k/training')\n",
    "# documents = [{'id': doc.doc_id, 'text': doc.text} for doc in dataset.docs_iter()]\n",
    "\n",
    "# # Initialize TfidfEngine with the TextPreprocessor\n",
    "# tfidf_engine = TfidfEngine(text_preprocessor)\n",
    "\n",
    "# # Train the TF-IDF model (uncomment to train and save the model)\n",
    "# # tfidf_engine.train_model(documents)\n",
    "\n",
    "# # Load the trained model\n",
    "# tfidf_engine.load_model()\n",
    "\n",
    "# # Calculate MAP for all queries in the dataset\n",
    "# dataset = ir_datasets.load(\"wikir/en1k/training\")\n",
    "\n",
    "# queries_ids = {qrel.query_id for qrel in dataset.qrels_iter()}\n",
    "# map_sum = 0\n",
    "\n",
    "# for query_id in queries_ids:\n",
    "#     map_sum += calculate_MAP(query_id, tfidf_engine, dataset)\n",
    "\n",
    "# mean_average_precision = map_sum / len(queries_ids)\n",
    "# print(f\"Mean Average Precision (MAP): {mean_average_precision}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Average Precision (MAP): 0.4649909229876424\n",
      "Mean Precision@10: 0.13614958448753353\n",
      "Mean Recall: 0.1197556803665943\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "def calculate_MAP(query_id, tfidf_engine, dataset):\n",
    "    relevant_docs = [qrel.doc_id for qrel in dataset.qrels_iter() if qrel.query_id == query_id]\n",
    "    \n",
    "    ordered_results = []\n",
    "    for query in dataset.queries_iter():\n",
    "        if query.query_id == query_id:\n",
    "            ordered_results = tfidf_engine.get_results(query.text)\n",
    "            break\n",
    "\n",
    "    pk_sum = 0\n",
    "    total_relevant = 0\n",
    "    for i in range(1, 11):\n",
    "        relevant_ret = 0\n",
    "        for j in range(i):\n",
    "            if j < len(ordered_results) and ordered_results[j]['_id'] in relevant_docs:\n",
    "                relevant_ret += 1\n",
    "        p_at_k = (relevant_ret / i) * (1 if i-1 < len(ordered_results) and ordered_results[i-1]['_id'] in relevant_docs else 0)\n",
    "        pk_sum += p_at_k\n",
    "        if i-1 < len(ordered_results) and ordered_results[i-1]['_id'] in relevant_docs:\n",
    "            total_relevant += 1\n",
    "\n",
    "    return 0 if total_relevant == 0 else pk_sum / total_relevant\n",
    "def calculate_precision_at_10(query_id, tfidf_engine, dataset):\n",
    "    relevant_docs = [qrel.doc_id for qrel in dataset.qrels_iter() if qrel.query_id == query_id]\n",
    "    \n",
    "    ordered_results = []\n",
    "    for query in dataset.queries_iter():\n",
    "        if query.query_id == query_id:\n",
    "            ordered_results = tfidf_engine.get_results(query.text)\n",
    "            break\n",
    "\n",
    "    retrieved_docs = [result['_id'] for result in ordered_results[:10]]\n",
    "    relevant_retrieved_docs = [doc for doc in retrieved_docs if doc in relevant_docs]\n",
    "\n",
    "    precision_at_10 = len(relevant_retrieved_docs) / 10\n",
    "    return precision_at_10\n",
    "\n",
    "def calculate_recall(query_id, tfidf_engine, dataset):\n",
    "    relevant_docs = [qrel.doc_id for qrel in dataset.qrels_iter() if qrel.query_id == query_id]\n",
    "    \n",
    "    ordered_results = []\n",
    "    for query in dataset.queries_iter():\n",
    "        if query.query_id == query_id:\n",
    "            ordered_results = tfidf_engine.get_results(query.text)\n",
    "            break\n",
    "\n",
    "    retrieved_docs = [result['_id'] for result in ordered_results]\n",
    "    relevant_retrieved_docs = [doc for doc in retrieved_docs if doc in relevant_docs]\n",
    "\n",
    "    recall = len(relevant_retrieved_docs) / len(relevant_docs) if relevant_docs else 0\n",
    "    return recall\n",
    "\n",
    "# # Initialize TextPreprocessor\n",
    "text_preprocessor = TextPreprocessor()\n",
    "\n",
    "# Load documents\n",
    "dataset = ir_datasets.load('wikir/en1k/training')\n",
    "documents = [{'id': doc.doc_id, 'text': doc.text} for doc in dataset.docs_iter()]\n",
    "\n",
    "# # Initialize TfidfEngine with the TextPreprocessor\n",
    "tfidf_engine = TfidfEngine(text_preprocessor)\n",
    "\n",
    "# Train the TF-IDF model (uncomment to train and save the model)\n",
    "# tfidf_engine.train_model(documents)\n",
    "\n",
    "# Load the trained model\n",
    "tfidf_engine.load_model()\n",
    "\n",
    "# Calculate MAP, precision@10, and recall for all queries in the dataset\n",
    "dataset = ir_datasets.load(\"wikir/en1k/training\")\n",
    "\n",
    "queries_ids = {qrel.query_id for qrel in dataset.qrels_iter()}\n",
    "map_sum = 0\n",
    "precision_at_10_sum = 0\n",
    "recall_sum = 0\n",
    "\n",
    "for query_id in queries_ids:\n",
    "    map_sum += calculate_MAP(query_id, tfidf_engine, dataset)\n",
    "    precision_at_10_sum += calculate_precision_at_10(query_id, tfidf_engine, dataset)\n",
    "    recall_sum += calculate_recall(query_id, tfidf_engine, dataset)\n",
    "\n",
    "mean_average_precision = map_sum / len(queries_ids)\n",
    "mean_precision_at_10 = precision_at_10_sum / len(queries_ids)\n",
    "mean_recall = recall_sum / len(queries_ids)\n",
    "\n",
    "print(f\"Mean Average Precision (MAP): {mean_average_precision}\")\n",
    "print(f\"Mean Precision@10: {mean_precision_at_10}\")\n",
    "print(f\"Mean Recall: {mean_recall}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
